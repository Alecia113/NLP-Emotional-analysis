{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZrvo/S5vrzJ7GRXq9gp03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/zong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCXc9lTdz1Dn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx_SlyzHoIf-"
      },
      "source": [
        "# Please comment your code\n",
        "#[lab5]\n",
        "import torch\n",
        "from random import shuffle\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Separate data and labels into two lists. As it is the given data set are in the form of data + labels, so to separate it\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))]         #neg pos\n",
        "\n",
        "\n",
        "# [lab3]random training set To make the model trained better, because the training set given in front of the front is separated before and after 000111, the front is all negative after all positive\n",
        "# It seems that it is okay not to add it, but the accuracy is improved by adding it. Before adding is to adjust the training set according to the batch_size, otherwise it will appear when training the model, the accuracy of 0.5/1 constant situation. Although the loss is changing, the classification situation remains the same.\n",
        "zipped = zip(train_data,train_label)                          #The zip function can only be used once. If you print(list(zipped)) directly, zipped will be empty afterwards, because the iterator has already been used once; if you want to reuse the result, you need to save it to a list.\n",
        "Zipp = list(zipped) \n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "def preprocessing(data_text):\n",
        "  text = [s.lower() for s in data_text]                       #do preprocessing to lowercase [lab5]. Because it is easier to match elements in the dictionary if they are lowercase. It will not be the case that some words are lowercase and some are uppercase, which makes the data set more neat and makes it easier to delete words for subsequent processing.\n",
        "  '''\n",
        "  def remove_punctuation_re(x):                   #Delete the punctuation. At the very beginning I first used the keep expressions method for deleting punctuation. In the quotes below, if you want to try it, comment out the delete all punctuation and turn on the comment symbol for the remove method below.\n",
        "    x1 = re.sub(r'[^\\w\\s]','',x)\n",
        "    x2 = re.sub(r'\\d','',x1)\n",
        "    return x2\n",
        "\n",
        "  text_re = [remove_punctuation_re(s) for s in text]\n",
        "  '''\n",
        "  tknzr = TweetTokenizer()\n",
        "  text_t=[]                                    #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "  for s in text:                        #text ==text_re\n",
        "    text = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "    text_t.append(text)  \n",
        "    \n",
        "                                           #'''表情的保留方式  因为字典中没表情，所以就不用表情了\n",
        "  def remove(x):\n",
        "    t = []\n",
        "    for i in range(len(x)):\n",
        "      t_sub = []                            #是直接用空列表代替了\n",
        "      for j in range(len(x[i])):\n",
        "        if len(x[i][j])==0:\n",
        "          continue\n",
        "        if x[i][j] == \" \":\n",
        "          continue \n",
        "        else:\n",
        "          x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "          if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "            x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "          if len(x[i][j]) == 0:\n",
        "            continue \n",
        "          else:\n",
        "            t_sub.append(x[i][j]) \n",
        "      t.append(t_sub) \n",
        "    return t\n",
        "\n",
        "  new_text = remove(text_t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  stop_words = sw.words()\n",
        "  sww = sw.words()\n",
        "  text_stop=[]    #8000\n",
        "  for tokens in new_text:             #new_text == text_t\n",
        "      filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "      text_stop.append(filtered_sentence)\n",
        "  return text_stop\n",
        "\n",
        "pre_train= preprocessing(train_data)  #对应后面，后面还得改一下 train_stem ;test_stem\n",
        "pre_test =preprocessing(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKauJyDUp07n"
      },
      "source": [
        "# Please comment your code\n",
        "#[lab5]\n",
        "import torch\n",
        "from random import shuffle\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Separate data and labels into two lists. As it is the given data set are in the form of data + labels, so to separate it\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))]         #neg pos\n",
        "\n",
        "\n",
        "# [lab3]random training set To make the model trained better, because the training set given in front of the front is separated before and after 000111, the front is all negative after all positive\n",
        "# It seems that it is okay not to add it, but the accuracy is improved by adding it. Before adding is to adjust the training set according to the batch_size, otherwise it will appear when training the model, the accuracy of 0.5/1 constant situation. Although the loss is changing, the classification situation remains the same.\n",
        "zipped = zip(train_data,train_label)                          #The zip function can only be used once. If you print(list(zipped)) directly, zipped will be empty afterwards, because the iterator has already been used once; if you want to reuse the result, you need to save it to a list.\n",
        "Zipp = list(zipped) \n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "def preprocessing(data_text):\n",
        "  text = [s.lower() for s in data_text]                       #do preprocessing to lowercase [lab5]. Because it is easier to match elements in the dictionary if they are lowercase. It will not be the case that some words are lowercase and some are uppercase, which makes the data set more neat and makes it easier to delete words for subsequent processing.\n",
        "\n",
        "  def remove_punctuation_re(x):                   #Delete the punctuation. At the very beginning I first used the keep expressions method for deleting punctuation. In the quotes below, if you want to try it, comment out the delete all punctuation and turn on the comment symbol for the remove method below.\n",
        "    x1 = re.sub(r'[^\\w\\s]','',x)\n",
        "    x2 = re.sub(r'\\d','',x1)\n",
        "    return x2\n",
        "\n",
        "  text_re = [remove_punctuation_re(s) for s in text]\n",
        "\n",
        "  tknzr = TweetTokenizer()\n",
        "  text_t=[]                                    #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "  for s in text_re:\n",
        "    text_re = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "    text_t.append(text_re)  \n",
        "    \n",
        "  '''表情的保留方式  因为字典中没表情，所以就不用表情了\n",
        "  def remove(x):\n",
        "    t = []\n",
        "    for i in range(len(x)):\n",
        "      t_sub = []        #是直接用空列表代替了\n",
        "      for j in range(len(x[i])):\n",
        "        if len(x[i][j])==0:\n",
        "          continue\n",
        "        if x[i][j] == \" \":\n",
        "          continue \n",
        "        else:\n",
        "          x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "          if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "            x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "          if len(x[i][j]) == 0:\n",
        "            continue \n",
        "          else:\n",
        "            t_sub.append(x[i][j]) \n",
        "      t.append(t_sub) \n",
        "    return t\n",
        "\n",
        "  new_text = remove(text_t)\n",
        "  '''\n",
        "\n",
        "\n",
        "\n",
        "  stop_words = sw.words()\n",
        "  sww = sw.words()\n",
        "  text_stop=[]    #8000\n",
        "  for tokens in text_t:\n",
        "      filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "      text_stop.append(filtered_sentence)\n",
        "  return text_stop\n",
        "\n",
        "pre_train= preprocessing(train_data)  #对应后面，后面还得改一下 train_stem ;test_stem\n",
        "pre_test =preprocessing(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rQaZuVdqeQ-"
      },
      "source": [
        "# Please comment your code\n",
        "#[lab5]\n",
        "import torch\n",
        "from random import shuffle\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Separate data and labels into two lists. As it is the given data set are in the form of data + labels, so to separate it\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))]         #neg pos\n",
        "\n",
        "\n",
        "# [lab3]random training set To make the model trained better, because the training set given in front of the front is separated before and after 000111, the front is all negative after all positive\n",
        "# It seems that it is okay not to add it, but the accuracy is improved by adding it. Before adding is to adjust the training set according to the batch_size, otherwise it will appear when training the model, the accuracy of 0.5/1 constant situation. Although the loss is changing, the classification situation remains the same.\n",
        "zipped = zip(train_data,train_label)                          #The zip function can only be used once. If you print(list(zipped)) directly, zipped will be empty afterwards, because the iterator has already been used once; if you want to reuse the result, you need to save it to a list.\n",
        "Zipp = list(zipped) \n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "def preprocessing(data_text):\n",
        "  text = [s.lower() for s in data_text]                       #do preprocessing to lowercase [lab5]. Because it is easier to match elements in the dictionary if they are lowercase. It will not be the case that some words are lowercase and some are uppercase, which makes the data set more neat and makes it easier to delete words for subsequent processing.\n",
        "\n",
        "  def remove_punctuation_re(x):                   #Delete the punctuation. At the very beginning I first used the keep expressions method for deleting punctuation. In the quotes below, if you want to try it, comment out the delete all punctuation and turn on the comment symbol for the remove method below.\n",
        "    x1 = re.sub(r'[^\\w\\s]','',x)                  #\n",
        "    x2 = re.sub(r'\\d','',x1)\n",
        "    return x2\n",
        "\n",
        "  text_re = [remove_punctuation_re(s) for s in text]\n",
        "\n",
        "  tknzr = TweetTokenizer()\n",
        "  text_t=[]                                    #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "  for s in text_re:\n",
        "    text_re = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "    text_t.append(text_re)  \n",
        "  '''表情的保留方式  因为字典中没表情，所以就不用表情了\n",
        "  def remove(x):\n",
        "    t = []\n",
        "    for i in range(len(x)):\n",
        "      t_sub = []        #是直接用空列表代替了\n",
        "      for j in range(len(x[i])):\n",
        "        if len(x[i][j])==0:\n",
        "          continue\n",
        "        if x[i][j] == \" \":\n",
        "          continue \n",
        "        else:\n",
        "          x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "          if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "            x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "          if len(x[i][j]) == 0:\n",
        "            continue \n",
        "          else:\n",
        "            t_sub.append(x[i][j]) \n",
        "      t.append(t_sub) \n",
        "    return t\n",
        "\n",
        "  new_text = remove(text_t)\n",
        "  '''\n",
        "\n",
        "\n",
        "\n",
        "  stop_words = sw.words()\n",
        "  sww = sw.words()\n",
        "  text_stop=[]    #8000\n",
        "  for tokens in text_t:\n",
        "      filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "      text_stop.append(filtered_sentence)\n",
        "  return text_stop\n",
        "\n",
        "pre_train= preprocessing(train_data)  #对应后面，后面还得改一下 train_stem ;test_stem\n",
        "pre_test =preprocessing(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWPbIh6dqlhz"
      },
      "source": [
        "'''\n",
        "# Please comment your code 评述你的代码 NLTK's Twitter_Sample dataset. \n",
        "# (Justify your decision) #[lab5]\n",
        "'''\n",
        "#[lab5]\n",
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#分集合;因为是给定的数据集都是数据+标签的形式，所以要把它分开\n",
        "\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))] #neg pos\n",
        "\n",
        "# 随机训练集 为了让模型训练的更好，因为给的训练集前面是前后分开的000111，前面都是消极后面都是积极\n",
        "#看起来不加也行，加了准确率就提升了。。；没加之前是得根据batch_size 来调整训练集，要不然会出现训练模型时候，准确度0.5/1恒定的情况。 虽然loss在变，但是分类的情况却没有变。\n",
        "#[lab3]\n",
        "zipped = zip(train_data,train_label)   #上图说明 b 调用一次之后，再调用的 c 则为空。如果想要复用这个结果，需要保存到 list 里，即：\n",
        "Zipp = list(zipped) #如果直接 print(list(zipped))，则之后 zipped为空，因为已经用完一次迭代器了；如果想要复用这个结果，需要保存到 list 里\n",
        "from random import shuffle\n",
        "\n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "#做预处理 变小写 [lab5] 因为小写的话更容易匹配字典中的元素。就不会出现有些小写有些大写的情况，让数据集更加工整，也为了后续处理删词什么的方便。\n",
        "import pprint\n",
        "text_train = [s.lower() for s in train_data]\n",
        "text_test = [s.lower() for s in test_data]\n",
        "\n",
        "# 分词[lab5] 因为就是需要一个个词的来处理。通过一个或者多个词来判断整个句子的关系，那么首先要判断每个词的关系。\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "\n",
        "train_t=[]  #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "for s in text_train:\n",
        "  text_train = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "  train_t.append(text_train)  \n",
        "        #:(    :-(\n",
        "\n",
        "\n",
        "test_t=[]     #2000\n",
        "for w in text_test:\n",
        "  text_test = tknzr.tokenize(w)\n",
        "  test_t.append(text_test)\n",
        "\n",
        "# 删标点，这里是保留了一部分表情。但是后来发现，因为字典中没有表情，所以可以不保留表情[lab5]\n",
        "#theprincesszooz but i see what youre going at   \n",
        "#yes  subjective pain may not be real\n",
        "#  but that does not make it less painful\n",
        "'''\n",
        "还得改\n",
        "clean_doc2 = re.sub(r'[^\\w\\s]','',corpus[1])字符\n",
        "clean_doc2 = re.sub(r'\\d,'',corpus[1])数字\n",
        "'''\n",
        "import re\n",
        "def remove(x):\n",
        "  t = []\n",
        "  for i in range(len(x)):\n",
        "    t_sub = []        #是直接用空列表代替了\n",
        "    for j in range(len(x[i])):\n",
        "      if len(x[i][j])==0:\n",
        "        continue\n",
        "      if x[i][j] == \" \":\n",
        "        continue \n",
        "      else:\n",
        "        x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "        if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "          x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "        if len(x[i][j]) == 0:\n",
        "          continue \n",
        "        else:\n",
        "          t_sub.append(x[i][j]) \n",
        "    t.append(t_sub) \n",
        "\n",
        "  return t\n",
        "\n",
        "text_train = remove(train_t)\n",
        "text_test = remove(test_t)\n",
        "\n",
        "##停用词  删除那些可有可无不影响句意比如a;an;the 但是又数目非常多的词。\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "sww = sw.words()\n",
        "\n",
        "text_train_le=[]    #8000\n",
        "for tokens in text_train:\n",
        "    filtered_sentence1 = [w for w in tokens if not w in stop_words]\n",
        "    text_train_le.append(filtered_sentence1)\n",
        "\n",
        "\n",
        "text_test_le=[]   #2000\n",
        "for tokens in text_test:\n",
        "  filtered_sentence2 = [w for w in tokens if not w in stop_words]\n",
        "  text_test_le.append(filtered_sentence2)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMD9V_XOqqEV"
      },
      "source": [
        "\n",
        "# stem\n",
        "from nltk.stem.porter import *\n",
        "def stemming(past_text):\n",
        "  stemmer = PorterStemmer()\n",
        "  train_stem = []\n",
        "  for i in range(len(past_text)):\n",
        "    singles = []\n",
        "    for plural in past_text[i]:\n",
        "      singles.append(stemmer.stem(plural))\n",
        "    train_stem.append(singles)\n",
        "  return train_stem\n",
        "\n",
        "stem_train = stemming(text_train_le)  #nobodies -->nobodi\n",
        "stem_test = stemming(text_test_le)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOuSkuM_wxlu"
      },
      "source": [
        "#主要是为了得到n_class\n",
        "unique_labels = np.unique(train_label)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "label_train_encoded = lEnc.fit_transform(train_label)         #lEnc.transform(unique_labels)[0]+1\n",
        "label_test_encoded = lEnc.fit_transform(test_label)\n",
        "n_class = len(unique_labels)                                   #主要是为了得到n_class  #n_class = np.unique(unique_labels).shape[0] #2相当于\n",
        "\n",
        "#要确定最大长度是多少 [lab4]\n",
        "doc_length_list = []                                              #会得到8000句话\n",
        "sum_text = sentences     #train_stem + test_stem要改的 sentences = stem_train + stem_test   \n",
        "maxlength = 0\n",
        "for doc in sum_text:\n",
        "    doc_length_list.append(len(doc))                                  #每句话多少个分词\n",
        "\n",
        "for index in range(len(doc_length_list)):\n",
        "  if doc_length_list[index] > maxlength:\n",
        "    maxlength = doc_length_list[index]\n",
        "    max_index = index\n",
        "#print(doc_length_list[max_index]) #21\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx76oanh1GTq"
      },
      "source": [
        "#According to 3.1, the effect of skip gram based on FastText is better.\n",
        "# Use hard coding for naming.\n",
        "sentences = stem_train + stem_test      # Facilitate training afterwards. Because the final model testing requires a training training set and a testing test set. This is done to facilitate some processing afterwards, such as supplementing the maximum length, etc. This way there is no need to change the overall code.\n",
        "SIZE= 100\n",
        "MIN_COUNT = 2                           #will led to OOV problems. Because My model ignores words with a word frequency lower than MIN_COUNT. So these words will not get the vector expression form.\n",
        "WINDOW = 5\n",
        "                                        #According to 3.1, it can be inferred that the size of SIZE does not have a significant effect on each word embedding model. It is fluctuating.\n",
        "\n",
        "from gensim.models import FastText\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "ft_sg_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "ft_sg_model.save('ft_sg.model')\n",
        "ft_sg = FastText.load(\"./ft_sg.model\")  \n",
        "\n",
        "#other model (word2vec)\n",
        "'''\n",
        "#gensim word2vec #W2V-cbow\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #open\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"   \n",
        "\n",
        "#FastText- sg\n",
        "\n",
        "ft_sg_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    \n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    \n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrdDK3Nu1Okz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "not_words = [w.strip() for w in open('/content/drive/MyDrive/NLP/A1/E/opinion-lexicon-English/negative-words.txt', 'r', encoding=\"ISO-8859-1\").readlines()]\n",
        "#处理文件 -\n",
        "j = -1\n",
        "for i in not_words:\n",
        "  j +=1\n",
        "  if i == '2-faced':\n",
        "    break\n",
        "neg = not_words[j:]\n",
        "\n",
        "#处理文件+\n",
        "sure_words = [w.strip() for w in open('/content/drive/MyDrive/NLP/A1/E/opinion-lexicon-English/positive-words.txt', 'r', encoding=\"ISO-8859-1\").readlines()]\n",
        "\n",
        "j = -1\n",
        "for i in sure_words:\n",
        "  j +=1\n",
        "  if i == 'a+':\n",
        "    break\n",
        "\n",
        "pos = sure_words[j:]\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "'''\n",
        "def lem(past_text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  text_le = []\n",
        "\n",
        "  lemma_sentence = [lemmatizer.lemmatize(w) for w in past_text ]\n",
        "  text_le.append(lemma_sentence)\n",
        "  return text_le\n",
        "neg_new = lem(neg)\n",
        "pos_new = lem(pos)\n",
        "'''\n",
        "\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "def stem(past_text):\n",
        "  stemmer = PorterStemmer()\n",
        "  text_le = []\n",
        "  stemmer_sentence = [stemmer.stem(plural) for plural in past_text ]\n",
        "  text_le.append(stemmer_sentence)\n",
        "  return text_le\n",
        "\n",
        "neg_new = stem(neg)\n",
        "pos_new = stem(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGntRAcV6VGS"
      },
      "source": [
        "#只是原来的，因为neg就是单词了。不能再neg[0]\n",
        "'''\n",
        "大改动\n",
        "'''\n",
        "def match(text):\n",
        "  sum_match= [] #train_match\n",
        "  for i in range(len(text)):  #train_stem  取出一句话\n",
        "    match = []\n",
        "    for j in range(len(text[i])):   #取出一句话中的单词\n",
        "      if text[i][j] in neg_new[0]:    #neg_stem  第几句话的第几个单词\n",
        "        match.append(1)   #neg 1\n",
        "      elif text[i][j] in pos_new[0]:  #pos_stem\n",
        "        match.append(2) #pos 2\n",
        "      else:\n",
        "        match.append(0)\n",
        "    sum_match.append(match)\n",
        "  return sum_match\n",
        "train_match = match(stem_train)  #这在不断的改变 train_stem  原本的话都是000\n",
        "test_match = match(stem_test)\n",
        "#只有stem好用\n",
        "#print(train_match)\n",
        "#print(test_match[:5])\n",
        "#就是把我原本的要训练的话，纷纷用012 表示出来。每个词告诉他是积极消极还是不在"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXdfsWZH9NfI"
      },
      "source": [
        "#实现的是concat的功能。\n",
        "'''\n",
        "大改动\n",
        "'''\n",
        "def input(text,text_match):\n",
        "  DF = {}\n",
        "\n",
        "  for tokensized_doc in text: \n",
        "      for term in np.unique(tokensized_doc):\n",
        "          try:\n",
        "              DF[term] +=1\n",
        "          except:\n",
        "              DF[term] =1\n",
        "  input = []\n",
        "  for w in range(len(text)):                #8000 0-7999\n",
        "    num = 0\n",
        "    new = []\n",
        "    for t in text[w]:                       #一句话 #t就是这个单词  #13\n",
        "      match = []                            #目前这个就是第一句话的match\n",
        "      if DF[t] >= 2:                         #0-12     这个min_count\n",
        "        word_vec = ft_sg[t].tolist()\n",
        "        match.append(text_match[w][num])     #w 012 []   #IndexError: list index out of range 后面的num问题\n",
        "        new_embedding = word_vec + match       #sent_embedding\n",
        "        new.append(new_embedding)\n",
        "      num += 1\n",
        "      if num >= len(text_match[w]):\n",
        "        break\n",
        "    input.append(new)                          # input == train_embedding\n",
        "                                                #print(input[0][0]) #input 8000一句话 3一个词 101 vec+0、1\n",
        "\n",
        "  return input\n",
        "\n",
        "input_train = input(stem_train,train_match)         #train_stem变化\n",
        "input_test = input(stem_test,test_match)           #test_stem变化\n",
        "#pprint.pprint(input_train[:1])\n",
        "#print('\\n')\n",
        "#pprint.pprint(input_test[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruJTBBCyAWGG"
      },
      "source": [
        "#测试用的删掉\n",
        "#主要是为了得到n_class\n",
        "unique_labels = np.unique(train_label)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "label_train_encoded = lEnc.fit_transform(train_label)         #lEnc.transform(unique_labels)[0]+1\n",
        "label_test_encoded = lEnc.fit_transform(test_label)\n",
        "n_class = len(unique_labels)                                   #主要是为了得到n_class  #n_class = np.unique(unique_labels).shape[0] #2相当于\n",
        "\n",
        "#要确定最大长度是多少 [lab4]\n",
        "doc_length_list = []                                              #会得到8000句话\n",
        "sum_text = sentences     #train_stem + test_stem要改的sentences = stem_train + stem_test   \n",
        "maxlength = 0\n",
        "for doc in sum_text:\n",
        "    doc_length_list.append(len(doc))                                  #每句话多少个分词\n",
        "\n",
        "for index in range(len(doc_length_list)):\n",
        "  if doc_length_list[index] > maxlength:\n",
        "    maxlength = doc_length_list[index]\n",
        "    max_index = index\n",
        "#print(doc_length_list[max_index]) #21\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALdOGgnzCLMR"
      },
      "source": [
        "#超参\n",
        "\n",
        "seq_length = doc_length_list[max_index]           #（22）应该是我规定的最大序列长度512那种  maxlength 增加减少那种 #一句话最长多少个词\n",
        "n_input = len(input_train[0][0])              #101 无所谓的反正都是101维度\n",
        "n_class = len(unique_labels)                  #2\n",
        "\n",
        "n_hidden = 200                                  #128\n",
        "batch_size = 800                                #500 \n",
        "total_epoch = 20                                #10\n",
        "learning_rate = 0.001                           # 1e-4 == 0.001 le-3 == 0.01 #0.1  0.05\n",
        "\n",
        "\n",
        "#把输入的规格都定成一样的， 变成句子长度是22，单词维度是101\n",
        "\n",
        "def sent_encoded(input_text):\n",
        "  sent_encoded = []\n",
        "  for m in range(len(input_text)):                       # m 1-8000 #训练集  input_train  input_test\n",
        "    \n",
        "    encoded = [] \n",
        "    zero = []\n",
        "    if len(input_text[m]) < seq_length:\n",
        "      zero = [len(input_text[0][0])*[0]] * (seq_length - len(input_text[m]))#补充到22 最长 # 不单单加0； 还是需要变成101维度\n",
        "      encoded = input_text[m] + zero\n",
        "    else:\n",
        "      encoded = input_text[m]\n",
        "    sent_encoded.append(encoded)                                              #要重新添加。 句子已经全变成22了 \n",
        "\n",
        "  sent_encoded = np.array(sent_encoded)\n",
        "  return sent_encoded\n",
        "\n",
        "sent_encoded_train = sent_encoded(input_train)\n",
        "sent_encoded_test = sent_encoded(input_test)\n",
        "#print(sent_encoded_train[:1]) #(8000, 22, 101)\n",
        "#print(sent_encoded_test[:1])#(2000, 22, 101)\n",
        "\n",
        "\n",
        "#还是Bi-LSTM准确度更高，因为LSTM处理了记忆丢失的问题。\n",
        "'''\n",
        "要写个LSTM比RNN好在哪里。\n",
        "'''\n",
        "# 模型Bi-RNN\n",
        "\n",
        "\n",
        "'''\n",
        "class Bi_RNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_RNN_Model, self).__init__()\n",
        "        # set the bidirectional to True\n",
        "        self.rnn = nn.RNN(n_input, n_hidden, batch_first =True, bidirectional=True) #, dropout = 0.5\n",
        "        self.linear = nn.Linear(2*n_hidden,n_class) #n_class = 3 \n",
        "        #https://stackoverflow.com/questions/60259836/cnn-indexerror-target-2-is-out-of-bounds 虽然是01分类但是，pytorch 要进行012 \n",
        "    def forward(self, x):        \n",
        "        x, h_n = self.rnn(x)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        output = self.linear(hidden_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "#LSTM-BI\n",
        "class Bi_LSTM_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_LSTM_Model, self).__init__()\n",
        "        #self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        # Initialize the Embedding layer with the lookup table we created \n",
        "        #self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "        # Optional: set requires_grad = False to make this lookup table untrainable\n",
        "        #self.emb.weight.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True, bidirectional=True)\n",
        "        self.linear = nn.Linear(n_hidden*2, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the embeded tensor\n",
        "        #x = self.emb(x)        \n",
        "        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
        "        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "        lstm_out, (h_n,c_n) = self.lstm(x)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        return z\n",
        "\n",
        "\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "'''\n",
        "#Bi-LSTM\n",
        "# Move the model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_LSTM_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVWYLpJ4I9DE"
      },
      "source": [
        "# Please comment your code\n",
        "#[lab4][lab5]\n",
        "#检查输出节点数与标签数是否一致。\n",
        "#并且将输出节点数num_outputs更改为标签数。\n",
        "# 因为我之前设置的class 类别是 0，1；所以这里也需要改成0，1 要不然后面会出现 target超出范围的错误。\n",
        "def label(label):   \n",
        "  label = np.array(label) #train_label\n",
        "  #neg 1 pos2\n",
        "  lab = []  #label_train\n",
        "  for tag in label:\n",
        "    if tag == 'neg':\n",
        "      lab.append(0)\n",
        "    else:\n",
        "      lab.append(1)\n",
        "  #print(label_train)\n",
        "  #print(len(label_train))\n",
        "  lab = np.array(lab)\n",
        "  #test_label\n",
        "  return lab\n",
        "\n",
        "label_train = label(train_label)\n",
        "label_test = label(test_label)\n",
        "\n",
        "#要开始训练。 optimizer +loss+ backward\n",
        "\n",
        "#处理\n",
        "#sent_encoded_train = sent_encoded(input_train)\n",
        "#sent_encoded_test = sent_encoded(input_test)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,sent_encoded_train.shape[0],batch_size):\n",
        "        input_batch = sent_encoded_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])] #256,22,101\n",
        "        target_batch = label_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])]  #256\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)  #256,22,101\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device) #256\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch)   #13,512,25 #256，22，101    #256 2 #500,22,101\n",
        "        loss = criterion(outputs, target_batch_torch) #256 2    256 out,y\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, 1)  # 输出值为元祖获取第二个tensor        \n",
        "    acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "    print('Epoch: %d, train loss: %.5f, train_acc:%.2f'%(epoch + 1, train_loss, acc))\n",
        "\n",
        "print('Finished Training')\n",
        "#这块是在后面评估，可删\n",
        "## Prediction\n",
        "\n",
        "model.eval()\n",
        "outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "predicted2 = torch.argmax(outputs2, 1)\n",
        "\n",
        "\n",
        "print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "#是改变了batch\n",
        "#3.59s"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
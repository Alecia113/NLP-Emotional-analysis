{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRmYspsbvlHzezFtSiuc01",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_nDkjR3ndsO",
        "outputId": "3d5c311a-4dcc-4458-87ba-e65234cd583b"
      },
      "source": [
        "# Please comment your code\n",
        "#要用语义，句法词关系来测试。[lab5-word_analogy_evaluation]\n",
        "#https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=EVk7tjwvhl-6\n",
        "#downloading xxx scripts from google drive\n",
        "import numpy as np\n",
        "import re\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n",
        "\n",
        "\n",
        "#data preprocessing\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='ISO-8859-1')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "#print(sentences[:10])\n",
        "\n",
        "\n",
        "#gensim word2vec\n",
        "#W2V-cbow\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "\n",
        "\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "\n",
        "#cbow_ted = Word2Vec.load(\"./ted_cbow_w2v.txt\")  # \"./存的名\"\n",
        "\n",
        "#打开训练好的文件\n",
        "vectors_file=\"/content/ted_cbow_w2v.txt\"\n",
        "#匹配\n",
        "with open(vectors_file, 'r') as f:\n",
        "  vectors = {}\n",
        "  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "    vals = line.rstrip().split(' ')\n",
        "    vectors[vals[0]] = [x for x in vals[1:]]\n",
        "\n",
        "#处理\n",
        "vocab_words=list(vectors.keys())\n",
        "vocab_size = len(vocab_words)\n",
        "print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "# create word->index and index->word converter  \n",
        "vocab = {w: idx for idx, w in enumerate(vocab_words)} #每个单词对应个index\n",
        "ivocab = {idx: w for idx, w in enumerate(vocab_words)}  #每个index对应一个单词\n",
        "\n",
        "\n",
        "# create the embedding matrix of shape (vocab_size, dim)\n",
        "vector_dim = len(vectors[ivocab[0]])  #100\n",
        "W = np.zeros((vocab_size, vector_dim))    #4325.100\n",
        "for word, v in vectors.items():\n",
        "    if word == '<unk>' or word == '':\n",
        "        continue\n",
        "    \n",
        "    W[vocab[word], :] = v   #100 102\n",
        "\n",
        "# normalize each word vector to unit length\n",
        "# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "W_norm = np.zeros(W.shape)\n",
        "d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "W_norm = (W.T / d).T\n",
        "\n",
        "#开始句法文本分析各种的功能语句。\n",
        "\n",
        "def evaluate_vectors(W, vocab, prefix='./content/GloVe/eval/question-data/'):#/content/GloVe/eval/question-data\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions  计算正确的语义问题\n",
        "    correct_syn = 0; # count correct syntactic questions 计算正确的句法问题\n",
        "    correct_tot = 0 # count correct questions   计算正确的问题\n",
        "    count_sem = 0; # count all semantic questions 计算所有语义问题\n",
        "    count_syn = 0; # count all syntactic questions  计算所有语法问题\n",
        "    count_tot = 0 # count all questions   计算所有问题  \n",
        "    full_count = 0 # count all questions, including those with unknown words    计算所有问题包括不知道n个单词的问题\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1) #216\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "\n",
        "#可视化展示\n",
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %    #看到的问题\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %  #语义准确度 绿色的\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' % #语法准确度 蓝色的\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
        "  #总准确度红线"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "fatal: destination path 'GloVe' already exists and is not an empty directory.\n",
            "Vocab size:  21613\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 8.09% (22/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 2.96% (15/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 0.73% (4/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 44.15% (151/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 1.97% (16/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 1.19% (6/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 31.68% (422/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 11.77% (89/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 35.08% (348/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 2.36% (20/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 22.13% (328/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 15.05% (140/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 16.93% (128/756)\n",
            "Questions seen/total: 51.72% (10109/19544)\n",
            "Semantic accuracy: 11.33%  (192/1694)\n",
            "Syntactic accuracy: 17.79%  (1497/8415)\n",
            "Total accuracy: 16.71%  (1689/10109)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
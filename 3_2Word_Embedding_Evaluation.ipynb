{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_2Word_Embedding_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/3_2Word_Embedding_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zZqGVn0e0Un",
        "outputId": "7a878b39-e849-40b8-bb3a-448ade5a76f1"
      },
      "source": [
        "#要用语义，句法词关系来测试。[lab5-word_analogy_evaluation]\n",
        "#https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=EVk7tjwvhl-6\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from lxml import etree \n",
        "import pprint\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n",
        "\n",
        "\n",
        "#data preprocessing\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='ISO-8859-1')\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "#print(sentences[:10])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAofhW3Dfsb3"
      },
      "source": [
        "#gensim word2vec #W2V-cbow\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xQy5nZBSRf"
      },
      "source": [
        "#gensim word2vec #W2V-cbow\n",
        "SIZE= 100\n",
        "MIN_COUNT = 5\n",
        "WINDOW = 5\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0SAd6AygEty",
        "outputId": "f4c6f833-6562-45f4-f257-12d546e832b3"
      },
      "source": [
        "#匹配\n",
        "def open_files(file_name):\n",
        "  with open(file_name, 'r') as f:    #vectors_file == vectors_wv_cbow  ==file_name\n",
        "    vectors = {}\n",
        "    for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "      vals = line.rstrip().split(' ')\n",
        "      vectors[vals[0]] = [x for x in vals[1:]]\n",
        "\n",
        "\n",
        "  vocab_words=list(vectors.keys())\n",
        "  vocab_size = len(vocab_words)\n",
        "  print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "  # create word->index and index->word converter  \n",
        "  vocab = {w: idx for idx, w in enumerate(vocab_words)} #每个单词对应个index\n",
        "  ivocab = {idx: w for idx, w in enumerate(vocab_words)}  #每个index对应一个单词\n",
        "\n",
        "\n",
        "  # create the embedding matrix of shape (vocab_size, dim)\n",
        "  vector_dim = len(vectors[ivocab[0]])  #100\n",
        "  W = np.zeros((vocab_size, vector_dim))    #4325.100\n",
        "  for word, v in vectors.items():\n",
        "      if word == '<unk>' or word == '':   #我加的\n",
        "          continue\n",
        "      \n",
        "      W[vocab[word], :] = v   #100 102\n",
        "\n",
        "  # normalize each word vector to unit length\n",
        "  # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "  W_norm = np.zeros(W.shape)\n",
        "  d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "  W_norm = (W.T / d).T\n",
        "  \n",
        "  return W, vocab, W_norm\n",
        "#W, vocab\n",
        "\n",
        "W1,vocab1,W1_norm = open_files(vectors_wv_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W2,vocab2,W2_norm = open_files(vectors_wv_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W3,vocab3,W3_norm = open_files(vectors_ft_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W4,vocab4,W4_norm = open_files(vectors_ft_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  21613\n",
            "Vocab size:  21613\n",
            "Vocab size:  21613\n",
            "Vocab size:  21613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfdTVCQbn9yU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfnxKECSn-6S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wARmZ2ifhxIE"
      },
      "source": [
        "\n",
        "#开始句法文本分析各种的功能语句。\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "def evaluate_vectors(W, vocab, prefix='./content/GloVe/eval/question-data/'):#/content/GloVe/eval/question-data\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions  计算正确的语义问题\n",
        "    correct_syn = 0; # count correct syntactic questions 计算正确的句法问题\n",
        "    correct_tot = 0 # count correct questions   计算正确的问题\n",
        "    count_sem = 0; # count all semantic questions 计算所有语义问题\n",
        "    count_syn = 0; # count all syntactic questions  计算所有语法问题\n",
        "    count_tot = 0 # count all questions   计算所有问题  \n",
        "    full_count = 0 # count all questions, including those with unknown words    计算所有问题包括不知道n个单词的问题\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1) #216\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "'''\n",
        "#可视化展示\n",
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %    #看到的问题\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %  #语义准确度 绿色的\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' % #语法准确度 蓝色的\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
        "  #总准确度红线\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljenv_HXk7cm"
      },
      "source": [
        "# 1-vectors_wv_cbow； 2-vectors_wv_sg； 3-vectors_ft_sg； 4vectors_ft_cbow\n",
        "def show_answer(W_norm,vocab):\n",
        "  #可视化展示\n",
        "  correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "  Semantic = 100 * correct_sem / float(count_sem)\n",
        "  Syntactic = 100 * correct_syn / float(count_syn)\n",
        "  Total = 100 * correct_tot / float(count_tot)\n",
        "  return Semantic, Syntactic, Total\n",
        "  #overall\n",
        "Semantic_wv_cbow, Syntactic_wv_cbow, Total_wv_cbow = show_answer(W1_norm, vocab1)\n",
        "print(Total_wv_cbow)\n",
        "print('\\n')\n",
        "Semantic_wv_sg, Syntactic_wv_sg, Total_wv_sg = show_answer(W2_norm, vocab2)\n",
        "print(Total_wv_sg)\n",
        "print('\\n')\n",
        "Semantic_ft_sg, Syntactic_ft_sg, Total_ft_sg = show_answer(W3_norm, vocab3)\n",
        "print(Total_ft_sg)\n",
        "print('\\n')\n",
        "Semantic_ft_cbow, Syntactic_ft_cbow, Total_ft_cbow = show_answer(W4_norm, vocab4)\n",
        "print(Total_ft_cbow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjZBL7kdsLIk"
      },
      "source": [
        "SIZE= 100\n",
        "MIN_COUNT = 5\n",
        "WINDOW = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvoKYOAosK9Q",
        "outputId": "c6adf7b0-a3e7-484b-dc91-eb74a1546c16"
      },
      "source": [
        "#table\n",
        "from prettytable import PrettyTable\n",
        "x = PrettyTable(['Model','Size','Window','Min_count','|','Sem','Syn','Tot'])\n",
        "x.align['Model'] = '1' #以model字段左对齐\n",
        "x.padding_width = 1 #填充快读\n",
        "x.add_row(['W2V_cbow','100','5','5','|',Semantic_wv_cbow, Syntactic_wv_cbow, Total_wv_cbow])\n",
        "x.add_row(['W2V_sg  ','100','5','5','|',Semantic_wv_sg, Syntactic_wv_sg, Total_wv_sg])\n",
        "x.add_row(['Ft_sg   ','100','5','5','|',Semantic_ft_sg, Syntactic_ft_sg, Total_ft_sg])\n",
        "x.add_row(['Ft_cbow ','100','5','5','|',Semantic_ft_cbow, Syntactic_ft_cbow, Total_ft_cbow])\n",
        "print(x)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------+--------+-----------+---+--------------------+--------------------+--------------------+\n",
            "|  Model   | Size | Window | Min_count | | |        Sem         |        Syn         |        Tot         |\n",
            "+----------+------+--------+-----------+---+--------------------+--------------------+--------------------+\n",
            "| W2V_cbow | 100  |   5    |     5     | | | 10.566706021251475 | 17.694592988710635 | 16.50014838262934  |\n",
            "| W2V_sg   | 100  |   5    |     5     | | | 12.750885478158205 | 20.80808080808081  | 19.457908794143833 |\n",
            "| Ft_sg    | 100  |   5    |     5     | | |  8.9728453364817   | 66.95187165775401  | 57.23612622415669  |\n",
            "| Ft_cbow  | 100  |   5    |     5     | | | 4.545454545454546  | 56.79144385026738  | 48.036403205064794 |\n",
            "+----------+------+--------+-----------+---+--------------------+--------------------+--------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTVo6bR12RO5",
        "outputId": "d559beeb-3f0d-4990-979f-f905254ee3d5"
      },
      "source": [
        "#Ft_sg\n",
        "'''\n",
        "第一个图：window = 10； size【0-600】（x） 把后三个准确率是y  【0，100,200,300,400,500,600]\n",
        "第二个图：size = 100 ； window【2-10】 [2.4.6,8,10]\n",
        "第三个图没get到。\n",
        "'''\n",
        "size = [100,200,300,400,500,600] #x\n",
        "MIN_COUNT = 5\n",
        "WINDOW = 10\n",
        "Sem_ftsg = []\n",
        "Syn_ftsg = []\n",
        "Tot_ftsg = []\n",
        "for s_size in range(len(size)):\n",
        "  SIZE = size[s_size]\n",
        "  test_ftsg = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "  test_ftsg.wv.save_word2vec_format('test_ftsg.txt', binary=False)\n",
        "  vectors_ftsg=\"/content/test_ftsg.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "  W_test,vocab_test,W_test_norm = open_files(vectors_ftsg)\n",
        "  Semantic_ftsg, Syntactic_ftsg, Total_ftsg = show_answer(W_test_norm, vocab_test)\n",
        "  Sem_ftsg.append(Semantic_ftsg)\n",
        "  Syn_ftsg.append(Syntactic_ftsg)\n",
        "  Tot_ftsg.append(Total_ftsg)\n",
        "print(Sem_ftsg)\n",
        "print(Syn_ftsg)\n",
        "print(Tot_ftsg)\n",
        "#y=accuary\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  21613\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.74% (2/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 1.97% (10/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 3.12% (17/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 35.38% (121/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 72.78% (591/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 76.28% (386/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 79.05% (1053/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 86.24% (652/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 81.25% (806/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 46.53% (395/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 38.39% (569/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 70.75% (658/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 69.31% (524/756)\n",
            "Vocab size:  21613\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 1.10% (3/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 1.97% (10/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 2.75% (15/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 29.24% (100/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 78.69% (639/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 84.78% (429/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 81.23% (1082/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 90.87% (687/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 80.54% (799/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 56.65% (481/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 35.49% (526/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 73.76% (686/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 79.89% (604/756)\n",
            "Vocab size:  21613\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.74% (2/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 1.97% (10/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 2.20% (12/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 28.07% (96/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 78.08% (634/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 85.18% (431/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 81.16% (1081/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 91.67% (693/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 80.34% (797/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 55.36% (470/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 35.96% (533/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 72.15% (671/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 81.48% (616/756)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D70AsyxFEb53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#等差数列。\n",
        "x = np.linspace(0,600,100)  #100-600\n",
        "y1 = [Semantic for Semantic in Sem_ftsg]\n",
        "y2 = [Syntactic for Syntactic in Syn_ftsg]\n",
        "y3 = [Total for Total in Tot_ftsg]\n",
        "plt.plot(x,y1,'rs-',markersize =5,label = 'Semantic')\n",
        "plt.plot(x,y2,'m-',markersize =5,label = 'Syntactic')\n",
        "plt.plot(x,y3,'k-',markersize =5,label = 'Total')\n",
        "plt.xlabel('Vector Size(Dimension)')\n",
        "plt.ylabel('Accuracy[%]')\n",
        "plt.title('The relationship between word vectors and semantics')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FkmMxkDHYsw"
      },
      "source": [
        "Window size变  size 不变"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Axkoj-HaYw"
      },
      "source": [
        "#Ft_sg\n",
        "'''\n",
        "第一个图：window = 10； size【0-600】（x） 把后三个准确率是y  【0，100,200,300,400,500,600]\n",
        "第二个图：size = 100 ； window【2-10】 [2.4.6,8,10]\n",
        "第三个图没get到。\n",
        "'''\n",
        "SIZE = 100 #x\n",
        "MIN_COUNT = 5\n",
        "WINDOW = [2,4,6,8,10]\n",
        "Sem_win_ftsg = []\n",
        "Syn_win_ftsg = []\n",
        "Tot_win_ftsg = []\n",
        "for win in range(len(WINDOW)):\n",
        "  WINDOW = size[win]\n",
        "  test_ftsg = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "  test_ftsg.wv.save_word2vec_format('test_ftsg.txt', binary=False)\n",
        "  vectors_ftsg=\"/content/test_ftsg.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "  W_test,vocab_test,W_test_norm = open_files(vectors_ftsg)\n",
        "  Semantic_ftsg, Syntactic_ftsg, Total_ftsg = show_answer(W_test_norm, vocab_test)\n",
        "  Sem_win_ftsg.append(Semantic_ftsg)\n",
        "  Syn_win_ftsg.append(Syntactic_ftsg)\n",
        "  Tot_win_ftsg.append(Total_ftsg)\n",
        "print(Sem_win_ftsg)\n",
        "print(Syn_win_ftsg)\n",
        "print(Tot_win_ftsg)\n",
        "#y=accuary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyl4McytJB3_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#等差数列。\n",
        "x = np.linspace(2,10,2)  \n",
        "yw1 = [Semantic for Semantic in Sem_win_ftsg]\n",
        "yw2 = [Syntactic for Syntactic in Syn_win_ftsg]\n",
        "yw3 = [Total for Total in Tot_win_ftsg]\n",
        "plt.plot(x,yw1,'rs-',markersize =5,label = 'Semantic')\n",
        "plt.plot(x,yw2,'m-',markersize =5,label = 'Syntactic')\n",
        "plt.plot(x,yw3,'k-',markersize =5,label = 'Total')\n",
        "plt.xlabel('Window Size')\n",
        "plt.ylabel('Accuracy[%]')\n",
        "plt.title('The relationship between window size and semantics')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhWDNW6W2RUr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0POp5HgrIAu"
      },
      "source": [
        "W1,vocab1,W1_norm = open_files(vectors_wv_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W2,vocab2,W2_norm = open_files(vectors_wv_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W3,vocab3,#  x是epoch y是f1 测试集  command+/\n",
        "import matplotlib.pyplot as plt\n",
        "x = [epoch for epoch in range(total_epoch)]\n",
        "y7 = [f1 for f1 in line_lr[6]]\n",
        "\n",
        "plt.plot(x,y1,'rs-',markersize =3,label = 'lr=1') # 那个点  红色尺寸点\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('f1')\n",
        "plt.title('F1 cases at different learning rates.')\n",
        "plt.legend() #让标签显示出来\n",
        "plt.show()\n",
        "# precision: TP/(TP+FP)\n",
        "#recallW3_norm = open_files(vectors_ft_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W4,vocab4,W4_norm = open_files(vectors_ft_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J06JRYxmghJt"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'w2v_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV3NL8bBl3Qn"
      },
      "source": [
        "import torch\n",
        "data_w2v_cbow = torch.load('w2v_cbow.PATH')\n",
        "PP = data_w2v_cbow['correct_sem'] #19\n",
        "LL = data_w2v_cbow['correct_syn'] #0.6927\n",
        "P2 = data_w2v_cbow['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mb7R1ZDl4Hm"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'w2v_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2rh3qd0l4Nr"
      },
      "source": [
        "import torch\n",
        "data_w2v_sg = torch.load('w2v_sg.PATH')\n",
        "PP = data_w2v_sg['correct_sem'] #19\n",
        "LL = data_w2v_sg['correct_syn'] #0.6927\n",
        "P2 = data_w2v_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqg6i6LlrtpJ"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReqYnXrKrw_E"
      },
      "source": [
        "import torch\n",
        "data_ft_sg = torch.load('ft_sg.PATH')\n",
        "PP = data_ft_sg['correct_sem'] #19\n",
        "LL = data_ft_sg['correct_syn'] #0.6927\n",
        "P2 = data_ft_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zPleDRdr3sx"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_cbow.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX2hYsEFr5iY"
      },
      "source": [
        "import torch\n",
        "data_ft_cbow = torch.load('ft_cbow.PATH')\n",
        "PP = data_ft_cbow['correct_sem'] #19\n",
        "LL = data_ft_cbow['correct_syn'] #0.6927\n",
        "P2 = data_ft_cbow['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7qMXjre8B3"
      },
      "source": [
        ""
      ]
    }
  ]
}
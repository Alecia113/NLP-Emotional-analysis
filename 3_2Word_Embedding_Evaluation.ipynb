{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_2Word_Embedding_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/3_2Word_Embedding_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zZqGVn0e0Un",
        "outputId": "7a878b39-e849-40b8-bb3a-448ade5a76f1"
      },
      "source": [
        "#要用语义，句法词关系来测试。[lab5-word_analogy_evaluation]\n",
        "#https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=EVk7tjwvhl-6\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from lxml import etree \n",
        "import pprint\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n",
        "\n",
        "\n",
        "#data preprocessing\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='ISO-8859-1')\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "#print(sentences[:10])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAofhW3Dfsb3"
      },
      "source": [
        "#gensim word2vec #W2V-cbow\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xQy5nZBSRf"
      },
      "source": [
        "#gensim word2vec #W2V-cbow\n",
        "SIZE= 100\n",
        "MIN_COUNT = 5\n",
        "WINDOW = 5\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0SAd6AygEty",
        "outputId": "f4c6f833-6562-45f4-f257-12d546e832b3"
      },
      "source": [
        "#匹配\n",
        "def open_files(file_name):\n",
        "  with open(file_name, 'r') as f:    #vectors_file == vectors_wv_cbow  ==file_name\n",
        "    vectors = {}\n",
        "    for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "      vals = line.rstrip().split(' ')\n",
        "      vectors[vals[0]] = [x for x in vals[1:]]\n",
        "\n",
        "\n",
        "  vocab_words=list(vectors.keys())\n",
        "  vocab_size = len(vocab_words)\n",
        "  print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "  # create word->index and index->word converter  \n",
        "  vocab = {w: idx for idx, w in enumerate(vocab_words)} #每个单词对应个index\n",
        "  ivocab = {idx: w for idx, w in enumerate(vocab_words)}  #每个index对应一个单词\n",
        "\n",
        "\n",
        "  # create the embedding matrix of shape (vocab_size, dim)\n",
        "  vector_dim = len(vectors[ivocab[0]])  #100\n",
        "  W = np.zeros((vocab_size, vector_dim))    #4325.100\n",
        "  for word, v in vectors.items():\n",
        "      if word == '<unk>' or word == '':   #我加的\n",
        "          continue\n",
        "      \n",
        "      W[vocab[word], :] = v   #100 102\n",
        "\n",
        "  # normalize each word vector to unit length\n",
        "  # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "  W_norm = np.zeros(W.shape)\n",
        "  d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "  W_norm = (W.T / d).T\n",
        "  \n",
        "  return W, vocab, W_norm\n",
        "#W, vocab\n",
        "\n",
        "W1,vocab1,W1_norm = open_files(vectors_wv_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W2,vocab2,W2_norm = open_files(vectors_wv_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W3,vocab3,W3_norm = open_files(vectors_ft_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W4,vocab4,W4_norm = open_files(vectors_ft_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  21613\n",
            "Vocab size:  21613\n",
            "Vocab size:  21613\n",
            "Vocab size:  21613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfdTVCQbn9yU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfnxKECSn-6S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wARmZ2ifhxIE"
      },
      "source": [
        "\n",
        "#开始句法文本分析各种的功能语句。\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "def evaluate_vectors(W, vocab, prefix='./content/GloVe/eval/question-data/'):#/content/GloVe/eval/question-data\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions  计算正确的语义问题\n",
        "    correct_syn = 0; # count correct syntactic questions 计算正确的句法问题\n",
        "    correct_tot = 0 # count correct questions   计算正确的问题\n",
        "    count_sem = 0; # count all semantic questions 计算所有语义问题\n",
        "    count_syn = 0; # count all syntactic questions  计算所有语法问题\n",
        "    count_tot = 0 # count all questions   计算所有问题  \n",
        "    full_count = 0 # count all questions, including those with unknown words    计算所有问题包括不知道n个单词的问题\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1) #216\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "'''\n",
        "#可视化展示\n",
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %    #看到的问题\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %  #语义准确度 绿色的\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' % #语法准确度 蓝色的\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
        "  #总准确度红线\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljenv_HXk7cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae38089-cad2-450c-8878-7f885d8d978d"
      },
      "source": [
        "# 1-vectors_wv_cbow； 2-vectors_wv_sg； 3-vectors_ft_sg； 4vectors_ft_cbow\n",
        "def show_answer(W_norm,vocab):\n",
        "  #可视化展示\n",
        "  correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "  Semantic = 100 * correct_sem / float(count_sem)\n",
        "  Syntactic = 100 * correct_syn / float(count_syn)\n",
        "  Total = 100 * correct_tot / float(count_tot)\n",
        "  return Semantic, Syntactic, Total\n",
        "  #overall\n",
        "Semantic_wv_cbow, Syntactic_wv_cbow, Total_wv_cbow = show_answer(W1_norm, vocab1)\n",
        "print(Total_wv_cbow)\n",
        "print('\\n')\n",
        "Semantic_wv_sg, Syntactic_wv_sg, Total_wv_sg = show_answer(W2_norm, vocab2)\n",
        "print(Total_wv_sg)\n",
        "print('\\n')\n",
        "Semantic_ft_sg, Syntactic_ft_sg, Total_ft_sg = show_answer(W3_norm, vocab3)\n",
        "print(Total_ft_sg)\n",
        "print('\\n')\n",
        "Semantic_ft_cbow, Syntactic_ft_cbow, Total_ft_cbow = show_answer(W4_norm, vocab4)\n",
        "print(Total_ft_cbow)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 5.15% (14/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 2.37% (12/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 0.37% (2/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 44.15% (151/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 2.71% (22/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 0.59% (3/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 32.73% (436/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 10.85% (82/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 34.58% (343/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 2.59% (22/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 19.91% (295/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 17.96% (167/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 15.74% (119/756)\n",
            "16.50014838262934\n",
            "\n",
            "\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 5.15% (14/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 2.56% (13/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 3.67% (20/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 49.42% (169/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 3.82% (31/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 2.57% (13/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 37.76% (503/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 18.39% (139/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 35.99% (357/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 2.47% (21/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 20.85% (309/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 24.19% (225/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 20.24% (153/756)\n",
            "19.457908794143833\n",
            "\n",
            "\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 1.84% (5/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 2.17% (11/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 1.28% (7/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 37.72% (129/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 80.17% (651/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 83.00% (420/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 77.55% (1033/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 87.43% (661/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 80.04% (794/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 38.40% (326/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 37.85% (561/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 67.10% (624/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 74.60% (564/756)\n",
            "57.23612622415669\n",
            "\n",
            "\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.74% (2/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 0.59% (3/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 3.57% (1/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 0.18% (1/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 20.47% (70/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 71.31% (579/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 86.17% (436/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 77.78% (1036/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 86.64% (655/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 63.61% (631/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 19.08% (162/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 16.94% (251/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 52.69% (490/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 71.30% (539/756)\n",
            "48.036403205064794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjZBL7kdsLIk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvoKYOAosK9Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0POp5HgrIAu"
      },
      "source": [
        "W1,vocab1,W1_norm = open_files(vectors_wv_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W2,vocab2,W2_norm = open_files(vectors_wv_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W3,vocab3,W3_norm = open_files(vectors_ft_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W4,vocab4,W4_norm = open_files(vectors_ft_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J06JRYxmghJt"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'w2v_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV3NL8bBl3Qn"
      },
      "source": [
        "import torch\n",
        "data_w2v_cbow = torch.load('w2v_cbow.PATH')\n",
        "PP = data_w2v_cbow['correct_sem'] #19\n",
        "LL = data_w2v_cbow['correct_syn'] #0.6927\n",
        "P2 = data_w2v_cbow['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mb7R1ZDl4Hm"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'w2v_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2rh3qd0l4Nr"
      },
      "source": [
        "import torch\n",
        "data_w2v_sg = torch.load('w2v_sg.PATH')\n",
        "PP = data_w2v_sg['correct_sem'] #19\n",
        "LL = data_w2v_sg['correct_syn'] #0.6927\n",
        "P2 = data_w2v_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqg6i6LlrtpJ"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReqYnXrKrw_E"
      },
      "source": [
        "import torch\n",
        "data_ft_sg = torch.load('ft_sg.PATH')\n",
        "PP = data_ft_sg['correct_sem'] #19\n",
        "LL = data_ft_sg['correct_syn'] #0.6927\n",
        "P2 = data_ft_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zPleDRdr3sx"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_cbow.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX2hYsEFr5iY"
      },
      "source": [
        "import torch\n",
        "data_ft_cbow = torch.load('ft_cbow.PATH')\n",
        "PP = data_ft_cbow['correct_sem'] #19\n",
        "LL = data_ft_cbow['correct_syn'] #0.6927\n",
        "P2 = data_ft_cbow['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7qMXjre8B3"
      },
      "source": [
        ""
      ]
    }
  ]
}
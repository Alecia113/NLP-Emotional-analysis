{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_2Word_Embedding_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/3_2Word_Embedding_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zZqGVn0e0Un",
        "outputId": "1f0b65f9-0c6b-47ef-f734-d7662971c91a"
      },
      "source": [
        "#要用语义，句法词关系来测试。[lab5-word_analogy_evaluation]\n",
        "#https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=EVk7tjwvhl-6\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from lxml import etree \n",
        "import pprint\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n",
        "\n",
        "\n",
        "#data preprocessing\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='ISO-8859-1')\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "#print(sentences[:10])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAofhW3Dfsb3"
      },
      "source": [
        "#gensim word2vec #W2V-cbow\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0SAd6AygEty",
        "outputId": "0f33995b-0128-495f-a862-4b316d0b4015"
      },
      "source": [
        "#匹配\n",
        "def open_files(file_name):\n",
        "  with open(file_name, 'r') as f:    #vectors_file == vectors_wv_cbow  ==file_name\n",
        "    vectors = {}\n",
        "    for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "      vals = line.rstrip().split(' ')\n",
        "      vectors[vals[0]] = [x for x in vals[1:]]\n",
        "\n",
        "\n",
        "  vocab_words=list(vectors.keys())\n",
        "  vocab_size = len(vocab_words)\n",
        "  print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "  # create word->index and index->word converter  \n",
        "  vocab = {w: idx for idx, w in enumerate(vocab_words)} #每个单词对应个index\n",
        "  ivocab = {idx: w for idx, w in enumerate(vocab_words)}  #每个index对应一个单词\n",
        "\n",
        "\n",
        "  # create the embedding matrix of shape (vocab_size, dim)\n",
        "  vector_dim = len(vectors[ivocab[0]])  #100\n",
        "  W = np.zeros((vocab_size, vector_dim))    #4325.100\n",
        "  for word, v in vectors.items():\n",
        "      if word == '<unk>' or word == '':   #我加的\n",
        "          continue\n",
        "      \n",
        "      W[vocab[word], :] = v   #100 102\n",
        "\n",
        "  # normalize each word vector to unit length\n",
        "  # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "  W_norm = np.zeros(W.shape)\n",
        "  d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "  W_norm = (W.T / d).T\n",
        "  \n",
        "  return W, vocab, W_norm\n",
        "#W, vocab\n",
        "\n",
        "#W,vocab = open_files(vectors_wv_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "#W,vocab,W_norm = open_files(vectors_wv_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "#W,vocab,W_norm = open_files(vectors_ft_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W,vocab,W_norm = open_files(vectors_ft_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  21613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wARmZ2ifhxIE",
        "outputId": "a7a07a5e-db08-4d9b-ba50-53618bf8c845"
      },
      "source": [
        "\n",
        "#开始句法文本分析各种的功能语句。\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "def evaluate_vectors(W, vocab, prefix='./content/GloVe/eval/question-data/'):#/content/GloVe/eval/question-data\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions  计算正确的语义问题\n",
        "    correct_syn = 0; # count correct syntactic questions 计算正确的句法问题\n",
        "    correct_tot = 0 # count correct questions   计算正确的问题\n",
        "    count_sem = 0; # count all semantic questions 计算所有语义问题\n",
        "    count_syn = 0; # count all syntactic questions  计算所有语法问题\n",
        "    count_tot = 0 # count all questions   计算所有问题  \n",
        "    full_count = 0 # count all questions, including those with unknown words    计算所有问题包括不知道n个单词的问题\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1) #216\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "\n",
        "#可视化展示\n",
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %    #看到的问题\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %  #语义准确度 绿色的\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' % #语法准确度 蓝色的\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
        "  #总准确度红线"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GloVe' already exists and is not an empty directory.\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 1.47% (4/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 1.38% (7/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 0.00% (0/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 21.35% (73/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 69.46% (564/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 87.55% (443/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 78.08% (1040/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 86.64% (655/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 66.13% (656/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 20.02% (170/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 18.35% (272/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 51.18% (476/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 71.16% (538/756)\n",
            "Questions seen/total: 51.72% (10109/19544)\n",
            "Semantic accuracy: 4.96%  (84/1694)\n",
            "Syntactic accuracy: 57.21%  (4814/8415)\n",
            "Total accuracy: 48.45%  (4898/10109)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljenv_HXk7cm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J06JRYxmghJt"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'w2v_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "HV3NL8bBl3Qn",
        "outputId": "f38704e9-1baa-4be6-d6a9-38589adee9cd"
      },
      "source": [
        "import torch\n",
        "data_w2v_cbow = torch.load('w2v_cbow.PATH')\n",
        "PP = data_w2v_cbow['correct_sem'] #19\n",
        "LL = data_w2v_cbow['correct_syn'] #0.6927\n",
        "P2 = data_w2v_cbow['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4a0eda948536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_w2v_cbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2v_cbow.PATH'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mPP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_w2v_cbow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct_sem'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_w2v_cbow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct_syn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#0.6927\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mP2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_w2v_cbow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count_sem'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m#tensor([0, 1, 0,  ..., 0, 1, 0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'w2v_cbow.PATH'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mb7R1ZDl4Hm"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'w2v_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2rh3qd0l4Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592df49b-46b2-4a9c-dd1a-6e151dccfe8b"
      },
      "source": [
        "import torch\n",
        "data_w2v_sg = torch.load('w2v_sg.PATH')\n",
        "PP = data_w2v_sg['correct_sem'] #19\n",
        "LL = data_w2v_sg['correct_syn'] #0.6927\n",
        "P2 = data_w2v_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "189\n",
            "1747\n",
            "1694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqg6i6LlrtpJ"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReqYnXrKrw_E",
        "outputId": "71502f88-9bff-42a6-8f7f-12c8b6c0c014"
      },
      "source": [
        "import torch\n",
        "data_ft_sg = torch.load('ft_sg.PATH')\n",
        "PP = data_ft_sg['correct_sem'] #19\n",
        "LL = data_ft_sg['correct_syn'] #0.6927\n",
        "P2 = data_ft_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "160\n",
            "5646\n",
            "1694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zPleDRdr3sx"
      },
      "source": [
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_cbow.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX2hYsEFr5iY"
      },
      "source": [
        "import torch\n",
        "data_ft_cbow = torch.load('ft_cbow.PATH')\n",
        "PP = data_ft_cbow['correct_sem'] #19\n",
        "LL = data_ft_cbow['correct_syn'] #0.6927\n",
        "P2 = data_ft_cbow['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7qMXjre8B3"
      },
      "source": [
        ""
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKs266MzTKbcHKPDuFV6zR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/AE2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Roz0RvKoRzyx",
        "outputId": "c76483de-9f23-479f-a6d1-4c54f6523815"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle #必要的\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\")) #必要的\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\")) #必要的\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data))) #8000\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data))) #2000\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n",
        "#LABEL: neg / SENTENCE: hopeless for tmr :("
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv0bLvwgR4Ku"
      },
      "source": [
        " import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM9TvCMISJdf"
      },
      "source": [
        "#先搞个label。再搞个sentence\n",
        "#train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "\n",
        "##print(train_data[0]) #hopeless for tmr :(\n",
        "##test:'@AndreaMarySmith very helpful .... Or will be once I stop crying :('\n",
        "'''\n",
        "train_data =[]\n",
        "for i in range(len(training_data)):\n",
        "  train_data.append(training_data[i][1])\n",
        "print(train_data[0]) \n",
        "#上下一样\n",
        "'''\n",
        "\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))] #neg pos\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBERdsXso-ec"
      },
      "source": [
        "#做预处理 变小写\n",
        "import pprint\n",
        "text_train = [s.lower() for s in train_data]\n",
        "text_test = [s.lower() for s in test_data]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRxtaQwGSSr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9d49b0-5477-4c76-85b7-76eeba59f2f5"
      },
      "source": [
        "\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "\n",
        "train_t=[]  #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "for s in text_train:\n",
        "  text_train = tknzr.tokenize(s)                         #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "  train_t.append(text_train)  \n",
        "pprint.pprint(train_t[:5])         #:(    :-(\n",
        "\n",
        "\n",
        "test_t=[]     #2000\n",
        "for w in text_test:\n",
        "  text_test = tknzr.tokenize(w)\n",
        "  test_t.append(text_test)\n",
        "pprint.pprint(text_test[:5])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['hopeless', 'for', 'tmr', ':('],\n",
            " ['everything',\n",
            "  'in',\n",
            "  'the',\n",
            "  'kids',\n",
            "  'section',\n",
            "  'of',\n",
            "  'ikea',\n",
            "  'is',\n",
            "  'so',\n",
            "  'cute',\n",
            "  '.',\n",
            "  'shame',\n",
            "  \"i'm\",\n",
            "  'nearly',\n",
            "  '19',\n",
            "  'in',\n",
            "  '2',\n",
            "  'months',\n",
            "  ':('],\n",
            " ['@hegelbon',\n",
            "  'that',\n",
            "  'heart',\n",
            "  'sliding',\n",
            "  'into',\n",
            "  'the',\n",
            "  'waste',\n",
            "  'basket',\n",
            "  '.',\n",
            "  ':('],\n",
            " ['“',\n",
            "  '@ketchburning',\n",
            "  ':',\n",
            "  'i',\n",
            "  'hate',\n",
            "  'japanese',\n",
            "  'call',\n",
            "  'him',\n",
            "  '\"',\n",
            "  'bani',\n",
            "  '\"',\n",
            "  ':(',\n",
            "  ':(',\n",
            "  '”',\n",
            "  'me',\n",
            "  'too'],\n",
            " ['dang', 'starting', 'next', 'week', 'i', 'have', '\"', 'work', '\"', ':(']]\n",
            "['@danieloconnel18', 'you', 'could', 'say', 'he']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkeFpirqqp12"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOIFSExkpaZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b3640c-0a3f-42fe-bbcb-d19c35a811ef"
      },
      "source": [
        "#theprincesszooz but i see what youre going at   \n",
        "#yes  subjective pain may not be real\n",
        "#  but that does not make it less painful\n",
        "\n",
        "import re\n",
        "def remove(x):\n",
        "  t = []\n",
        "  for i in range(len(x)):\n",
        "    t_sub = []        #是直接用空列表代替了\n",
        "    for j in range(len(x[i])):\n",
        "      if len(x[i][j])==0:\n",
        "        continue\n",
        "      if x[i][j] == \" \":\n",
        "        continue \n",
        "      else:\n",
        "        x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "        if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "          x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "        if len(x[i][j]) == 0:\n",
        "          continue \n",
        "        else:\n",
        "          t_sub.append(x[i][j]) \n",
        "    t.append(t_sub) \n",
        "\n",
        "  return t\n",
        "\n",
        "text_train = remove(train_t)\n",
        "text_test = remove(test_t)\n",
        "print(text_train[:5])\n",
        "#print('==')\n",
        "#pprint.pprint(test_t[:5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['hopeless', 'for', 'tmr', ':('], ['everything', 'in', 'the', 'kids', 'section', 'of', 'ikea', 'is', 'so', 'cute', 'shame', \"i'm\", 'nearly', 'in', 'months', ':('], ['hegelbon', 'that', 'heart', 'sliding', 'into', 'the', 'waste', 'basket', ':('], ['ketchburning', 'i', 'hate', 'japanese', 'call', 'him', 'bani', ':(', ':(', 'me', 'too'], ['dang', 'starting', 'next', 'week', 'i', 'have', 'work', ':(']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylL2OARqb-gQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878c5d21-adb8-4855-d700-95816de18727"
      },
      "source": [
        "#停用词  \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "sww = sw.words()\n",
        "\n",
        "text_train_ns=[]    #8000\n",
        "for tokens in text_train:\n",
        "    filtered_sentence1 = [w for w in tokens if not w in stop_words]\n",
        "    text_train_ns.append(filtered_sentence1)\n",
        "pprint.pprint(text_train_ns[:5])\n",
        "\n",
        "text_test_ns=[]   #2000\n",
        "for tokens in text_test:\n",
        "  filtered_sentence2 = [w for w in tokens if not w in stop_words]\n",
        "  text_test_ns.append(filtered_sentence2)\n",
        "pprint.pprint(text_test_ns[:5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[['hopeless', 'tmr', ':('],\n",
            " ['everything',\n",
            "  'kids',\n",
            "  'section',\n",
            "  'ikea',\n",
            "  'cute',\n",
            "  'shame',\n",
            "  \"i'm\",\n",
            "  'nearly',\n",
            "  'months',\n",
            "  ':('],\n",
            " ['hegelbon', 'heart', 'sliding', 'waste', 'basket', ':('],\n",
            " ['ketchburning', 'hate', 'japanese', 'call', 'bani', ':(', ':('],\n",
            " ['dang', 'starting', 'next', 'week', 'work', ':(']]\n",
            "[['andreamarysmith', 'helpful', '...', 'stop', 'crying', ':('],\n",
            " ['realyys_', 'otl', 'nevermind', ':(', 'least', 'got', 'jeon'],\n",
            " ['soon',\n",
            "  'tweeted',\n",
            "  'planted',\n",
            "  'claws',\n",
            "  'thigh',\n",
            "  'traction',\n",
            "  'zoomed',\n",
            "  'away',\n",
            "  ':('],\n",
            " ['luketothestars', 'damnit', ':('],\n",
            " ['klm',\n",
            "  'used',\n",
            "  'pry',\n",
            "  'pv',\n",
            "  '...',\n",
            "  'wish',\n",
            "  'could',\n",
            "  'relive',\n",
            "  'days',\n",
            "  'become',\n",
            "  'nyc',\n",
            "  'pv',\n",
            "  'buy',\n",
            "  'way',\n",
            "  'communicate',\n",
            "  'nyc',\n",
            "  'usa',\n",
            "  'klm',\n",
            "  ':(']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG1yior4h7iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f680491e-fb64-4d8f-fe42-00b51a372850"
      },
      "source": [
        "#找词根\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_train_le = []\n",
        "for tokens in text_train_ns:\n",
        "  lemma_sentence1 = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "  text_train_le.append(lemma_sentence1)\n",
        "pprint.pprint(text_train_le[:10])\n",
        "\n",
        "text_test_le = []\n",
        "for tokens in text_test_ns:\n",
        "  lemma_sentence2 = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "  text_test_le.append(lemma_sentence2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[['hopeless', 'tmr', ':('],\n",
            " ['everything',\n",
            "  'kid',\n",
            "  'section',\n",
            "  'ikea',\n",
            "  'cute',\n",
            "  'shame',\n",
            "  \"i'm\",\n",
            "  'nearly',\n",
            "  'month',\n",
            "  ':('],\n",
            " ['hegelbon', 'heart', 'sliding', 'waste', 'basket', ':('],\n",
            " ['ketchburning', 'hate', 'japanese', 'call', 'ban', ':(', ':('],\n",
            " ['dang', 'starting', 'next', 'week', 'work', ':('],\n",
            " ['oh', 'god', 'baby', 'face', ':(', 'httpstcofcwgvaki'],\n",
            " ['rileymcdonough', 'make', 'smile', ':('],\n",
            " ['fggstar',\n",
            "  'stuartthull',\n",
            "  'work',\n",
            "  'neighbour',\n",
            "  'motor',\n",
            "  'asked',\n",
            "  'said',\n",
            "  'hate',\n",
            "  'update',\n",
            "  'search',\n",
            "  ':(',\n",
            "  'httptcoxvmtuikwln'],\n",
            " [':(', 'tahuodyy', 'sialan', ':(', 'httpstcohvixcrl'],\n",
            " ['athabasca',\n",
            "  'glacier',\n",
            "  ':-(',\n",
            "  'athabasca',\n",
            "  'glacier',\n",
            "  'jasper',\n",
            "  'jaspernationalpark',\n",
            "  'alberta',\n",
            "  'explorealberta',\n",
            "  'httptcodzzdqmfcz']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H3_DbySiBDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9802d396-e654-47f2-e4db-b7e4099c0657"
      },
      "source": [
        "#去掉复数\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "#text_train_le = [text_train_le[i] for i in range(len(text_train_le))]\n",
        "#singles = [stemmer.stem(plural) for plural in text_train_le[i] for i in range(len(text_train_le))]\n",
        "for i in range(len(text_train_le)):\n",
        "  for plural in text_train_le[i]:\n",
        "    singles = stemmer.stem(plural)\n",
        "print(text_train_le[:5])\n",
        "\n",
        "#text_train_le = [text_train_le[i] for i in range(len(text_train_le))]\n",
        "#singles = [stemmer.stem(plural) for plural in text_train_le[i] for i in range(len(text_train_le))]\n",
        "for i in range(len(text_test_le)):\n",
        "  for plural in text_test_le[i]:\n",
        "    singles = stemmer.stem(plural)\n",
        "print(text_test_le[:5])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['hopeless', 'tmr', ':('], ['everything', 'kid', 'section', 'ikea', 'cute', 'shame', \"i'm\", 'nearly', 'month', ':('], ['hegelbon', 'heart', 'sliding', 'waste', 'basket', ':('], ['ketchburning', 'hate', 'japanese', 'call', 'ban', ':(', ':('], ['dang', 'starting', 'next', 'week', 'work', ':(']]\n",
            "[['andreamarysmith', 'helpful', '...', 'stop', 'cry', ':('], ['realyys_', 'otl', 'nevermind', ':(', 'least', 'got', 'jeon'], ['soon', 'tweeted', 'planted', 'claw', 'thigh', 'traction', 'zoomed', 'away', ':('], ['luketothestars', 'damnit', ':('], ['klm', 'used', 'pry', 'pv', '...', 'wish', 'could', 'relive', 'day', 'become', 'nyc', 'pv', 'buy', 'way', 'communicate', 'nyc', 'usa', 'klm', ':(']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHvkY2jyU6u7"
      },
      "source": [
        "#**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldDg5heH3Ivm"
      },
      "source": [
        "#build word embeddings model\n",
        "#下载 预训练 嵌入 pre_trained embedding\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF-CafOE3yjE"
      },
      "source": [
        "#创建嵌入查询表  embedding lookup table 【是传说中的W吗？】 【为什么要嵌入这个词表】\n",
        "import numpy as np\n",
        "emb_dim = word_emb_model.vector_size    #要训练的数据集的向量大小\n",
        "emb_table = []\n",
        "for i, word in enumerate(word_list):  #['[PAD]', '[UNKNOWN]', 'do', 'hate', 'i', 'it', 'like', 'love', 'not', 'that']\n",
        "  if word in word_emb_model:\n",
        "    emb_table.append(word_emb_model[word])\n",
        "  else:\n",
        "    emb_table.append([0]*emb_dim)\n",
        "emb_table = np.array(emb_table)\n",
        "print(emb_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-8Fa35ckWLK"
      },
      "source": [
        "#WordEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrXqisXg20zg"
      },
      "source": [
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyrcOxr-bl0x",
        "outputId": "2f67aa3d-4d50-40cb-9795-1cdf1fed44e8"
      },
      "source": [
        "# 下载Twitter data\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfE-WSYHb3ye",
        "outputId": "cd203082-0b4a-4308-be2d-8c96235b44e5"
      },
      "source": [
        "#look like\n",
        "twitter_samples.fileids()\n",
        "print(\"Negative LENGTH : \" +  str(len(twitter_samples.strings('negative_tweets.json'))) )\n",
        "print(\"Positive LENGTH : \" +  str(len(twitter_samples.strings('positive_tweets.json'))) )\n",
        "twitter_samples.strings('positive_tweets.json')[:2]\n",
        "twitter_samples.strings('negative_tweets.json')[:2]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative LENGTH : 5000\n",
            "Positive LENGTH : 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless for tmr :(',\n",
              " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3_AoUPwb9_T",
        "outputId": "95690382-df05-4ea9-89af-c214a563a305"
      },
      "source": [
        "#putting labels\n",
        "label = \"neg\"\n",
        "neg_dataset = [(label, instance) for instance in twitter_samples.strings('negative_tweets.json')]\n",
        "\n",
        "label = \"pos\"\n",
        "pos_dataset = [(label, instance) for instance in twitter_samples.strings('positive_tweets.json')]\n",
        "\n",
        "print(neg_dataset[:2])\n",
        "print(pos_dataset[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('neg', 'hopeless for tmr :('), ('neg', \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\")]\n",
            "[('pos', '#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'), ('pos', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0SJ2Rk5cadq",
        "outputId": "020f8189-9cb3-43b3-a9bf-9d18ab8b30ce"
      },
      "source": [
        "#splitting dataset 分隔\n",
        "\n",
        "training_data = neg_dataset[:4000] + pos_dataset[:4000]\n",
        "testing_data = neg_dataset[4000:] + pos_dataset[4000:]\n",
        "\n",
        "\n",
        "####### relatively small dataset, can be used at development phase #######\n",
        "dev_training_data = neg_dataset[:100] + pos_dataset[:100]\n",
        "dev_testing_data = neg_dataset[100:200] + pos_dataset[100:200]\n",
        "\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of dev training dataset: {0}\".format(len(dev_training_data)))\n",
        "print(\"Size of dev testing dataset: {0}\".format(len(dev_testing_data)))\n",
        "print(\"------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "Size of dev training dataset: 200\n",
            "Size of dev testing dataset: 200\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4cG0EdfciYP"
      },
      "source": [
        "#saving dataset into pickle files\n",
        "import pickle\n",
        "# write to cPickle\n",
        "pickle.dump(training_data, open( \"training_data.pkl\", \"wb\" ) )\n",
        "pickle.dump(testing_data, open( \"testing_data.pkl\", \"wb\" ) )\n",
        "\n",
        "####### relatively small dataset, can be used at development phase #######\n",
        "pickle.dump(dev_training_data, open( \"dev_training_data.pkl\", \"wb\" ) )\n",
        "pickle.dump(dev_testing_data, open( \"dev_testing_data.pkl\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjakOd6Zcny7"
      },
      "source": [
        "#loading dataset from pickel\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '11CfwnSn-nN66U7YtCfCiJxSh3neZmKvk'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('dev_training_data.pkl')  \n",
        "\n",
        "id = '1AjVLvsK2p25eRsWWCyIqgP7fDs78UrIv'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('dev_testing_data.pkl')  \n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "dev_training_data = pickle.load(open(\"dev_training_data.pkl\",\"rb\"))\n",
        "dev_testing_data = pickle.load(open(\"dev_testing_data.pkl\",\"rb\"))\n",
        "\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zvxuRHkbwb"
      },
      "source": [
        "'''\n",
        "# word embedding  2分 [lab2]\n",
        "  创建word embedding model(表现word vectors eg: word2vec-CBOW, word2vec-Skip gram, fastText + Glove[lab2])\n",
        "  input embedding of your sequence model\n",
        "  [lab3,4]使用了one-hot vectors 作为inputs 给长序列模型\n",
        "  【要求做的】\n",
        "    1.数据的预处理（word embeddings[lab2;还要做解释说明]） NLTK Twitter dataset ( section 1 提供了) or/and 任何数据集（TED talk , Google News)\n",
        "    2.创建训练模型（for word embeddings) 创建训练模型；需要说清楚超参数【lab2（dimension of embeddings; window size, learning rate,etc.】 \n",
        "        note 任何word embeddings model[lab2] (word2vec-CBOW, word2vec-Skip gram, fasttext, glove)【证明】\n",
        "    3.训练模型： train model\n",
        "\n",
        "      [从lab2,4,5随便选参数]embeddings_dimension ; learning_rate epochs;\n",
        "  [lab5 有模型构建]\n",
        "'''\n",
        "\n",
        "'''\n",
        "先变成词向量。\n",
        "1变成向量矩阵。\n",
        "句子长度不同。要做padding\n",
        "np.mean()求均值。\n",
        "\n",
        "模型： lab3\n",
        "RNN\n",
        "0-1 loss（这个好）或 交叉熵\n",
        "linear： 叫分类器。 输出俩是p还是n\n",
        "batch-size； hiddenstates层数；维度\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnUPTjjulDBPYWU3MovJXJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/1_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJFkZQzwtE9q",
        "outputId": "bfcc66b5-8a0d-401c-9531-edb55ca44121"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle #必要的\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\")) #必要的\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\")) #必要的\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data))) #8000\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data))) #2000\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n",
        "#LABEL: neg / SENTENCE: hopeless for tmr :("
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JEG1R2GsSXM",
        "outputId": "265df172-d059-4bbd-cc9d-ddccecdc3517"
      },
      "source": [
        "import torch\n",
        "from random import shuffle\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import re\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))] #neg pos\n",
        "\n",
        "\n",
        "zipped = zip(train_data,train_label)  \n",
        "Zipp = list(zipped) \n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "def preprocessing(data_text):\n",
        "  text = [s.lower() for s in data_text]\n",
        "\n",
        "  tknzr = TweetTokenizer()\n",
        "  text_t=[]                                    #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "  for s in text:\n",
        "    text = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "    text_t.append(text)  \n",
        "  \n",
        "  def remove(x):\n",
        "    t = []\n",
        "    for i in range(len(x)):\n",
        "      t_sub = []        #是直接用空列表代替了\n",
        "      for j in range(len(x[i])):\n",
        "        if len(x[i][j])==0:\n",
        "          continue\n",
        "        if x[i][j] == \" \":\n",
        "          continue \n",
        "        else:\n",
        "          x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "          if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "            x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "          if len(x[i][j]) == 0:\n",
        "            continue \n",
        "          else:\n",
        "            t_sub.append(x[i][j]) \n",
        "      t.append(t_sub) \n",
        "    return t\n",
        "\n",
        "  new_text = remove(text_t)\n",
        "\n",
        "\n",
        "  stop_words = sw.words()\n",
        "  sww = sw.words()\n",
        "  text_stop=[]    #8000\n",
        "  for tokens in new_text:\n",
        "      filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "      text_stop.append(filtered_sentence)\n",
        "  return text_stop\n",
        "\n",
        "pre_train= preprocessing(train_data)  #对应后面，后面还得改一下 train_stem ;test_stem\n",
        "pre_test =preprocessing(test_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbPgBxq1tuzx"
      },
      "source": [
        "pprint.pprint(pre_train[:5])\n",
        "pprint.pprint(pre_test[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkTujVlnwWJm"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_train_le = []\n",
        "for tokens in text_train_ns:\n",
        "  lemma_sentence1 = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "  text_train_le.append(lemma_sentence1)\n",
        "pprint.pprint(text_train_le[:5])\n",
        "print('\\n')\n",
        "'''\n",
        "text_test_le = []\n",
        "for tokens in text_test_ns:\n",
        "  lemma_sentence2 = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "  text_test_le.append(lemma_sentence2)\n",
        "pprint.pprint(text_test_le[:5])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGMeUX7YsUeS"
      },
      "source": [
        "TEXT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckXHWwB8nnvc"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#找词根\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_train_le = []\n",
        "for tokens in text_train_ns:\n",
        "  lemma_sentence1 = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "  text_train_le.append(lemma_sentence1)\n",
        "pprint.pprint(text_train_le[:5])\n",
        "print('\\n')\n",
        "text_test_le = []\n",
        "for tokens in text_test_ns:\n",
        "  lemma_sentence2 = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "  text_test_le.append(lemma_sentence2)\n",
        "pprint.pprint(text_test_le[:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#去掉复数 stemmer 词根不需要这部； 如果用就需要在neg和pos需要stemmer操作 保留是因为更加便捷。\n",
        "#如果只保留原型lec…………那个，无法与现有给定字典一一匹配。\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "#text_train_le = [text_train_le[i] for i in range(len(text_train_le))]\n",
        "#singles = [stemmer.stem(plural) for plural in text_train_le[i] for i in range(len(text_train_le))]\n",
        "train_stem = []\n",
        "for i in range(len(text_train_le)):\n",
        "  singles1 = []\n",
        "  for plural in text_train_le[i]:\n",
        "    singles1.append(stemmer.stem(plural))\n",
        "  train_stem.append(singles1)\n",
        "\n",
        "\n",
        "\n",
        "#print(train_stem[:5])\n",
        "\n",
        "test_stem = []\n",
        "for i in range(len(text_test_le)):\n",
        "  singles2 = []\n",
        "  for plural in text_test_le[i]:\n",
        "    singles2.append(stemmer.stem(plural))\n",
        "  test_stem.append(singles2)\n",
        "\n",
        "#print(test_stem[:5])\n",
        "\n",
        "\n",
        "'''\n",
        "import re\n",
        "def remove(x):\n",
        "  t = []\n",
        "  for i in range(len(x)):\n",
        "    t_sub = []        #是直接用空列表代替了\n",
        "    for j in range(len(x[i])):\n",
        "      if len(x[i][j])==0:\n",
        "        continue\n",
        "      if x[i][j] == \" \":\n",
        "        continue \n",
        "      else:\n",
        "        x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "        if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "          x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "        if len(x[i][j]) == 0:\n",
        "          continue \n",
        "        else:\n",
        "          t_sub.append(x[i][j]) \n",
        "    t.append(t_sub) \n",
        "  return t\n",
        "'''\n",
        "text_train = remove(train_t)\n",
        "text_test = remove(test_t)\n",
        "\n",
        "##停用词  删除那些可有可无不影响句意比如a;an;the 但是又数目非常多的词。\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "sww = sw.words()\n",
        "\n",
        "text_train_le=[]    #8000\n",
        "for tokens in text_train:\n",
        "    filtered_sentence1 = [w for w in tokens if not w in stop_words]\n",
        "    text_train_le.append(filtered_sentence1)\n",
        "\n",
        "\n",
        "text_test_le=[]   #2000\n",
        "for tokens in text_test:\n",
        "  filtered_sentence2 = [w for w in tokens if not w in stop_words]\n",
        "  text_test_le.append(filtered_sentence2)\n",
        "\n",
        "#去掉复数 stemmer 词根不需要这部； 如果用就需要在neg和pos需要stemmer操作 保留是因为更加便捷。\n",
        "#如果只保留原型lec…………那个，无法与现有给定字典一一匹配。\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "#text_train_le = [text_train_le[i] for i in range(len(text_train_le))]\n",
        "#singles = [stemmer.stem(plural) for plural in text_train_le[i] for i in range(len(text_train_le))]\n",
        "train_stem = []\n",
        "for i in range(len(text_train_le)):\n",
        "  singles1 = []\n",
        "  for plural in text_train_le[i]:\n",
        "    singles1.append(stemmer.stem(plural))\n",
        "  train_stem.append(singles1)\n",
        "\n",
        "#print(train_stem[:5])\n",
        "\n",
        "test_stem = []\n",
        "for i in range(len(text_test_le)):\n",
        "  singles2 = []\n",
        "  for plural in text_test_le[i]:\n",
        "    singles2.append(stemmer.stem(plural))\n",
        "  test_stem.append(singles2)\n",
        "\n",
        "#print(test_stem[:5])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "总.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GXgFpxIgl-_G",
        "qhAgWf_AmbZ8",
        "LNys5HOdISK-",
        "a4mpRpocePLN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/%E6%80%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayROP2moYiRk"
      },
      "source": [
        "#改名"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqlh9_NyznS-"
      },
      "source": [
        "#在这说明那个调用的获取两个文件的事情。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOxG0HOj0D8s"
      },
      "source": [
        "[XXX] = 讲座/实验参考\n",
        "说明你的决定）= 请在文件中说明你的决定/选择的理由。你必须在报告中用经验性的证据表明你的最终决定。\n",
        "解释性能）= 请解释性能的趋势，以及为什么会出现这样的趋势的原因（或你的意见）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUBjTqOz0Gs0"
      },
      "source": [
        "#必须要有来自哪个lab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyAsAYTC0A5y"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8o7UZxWf3Y"
      },
      "source": [
        "If you want to know how data has been saved in pickle file: see this [ipynb file](https://drive.google.com/file/d/1ZQUVBzgH7N2EbiyE3WTPx7JNe2eRTs36/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0006d730-33e5-4617-adf1-4258dba0bf77"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe"
      },
      "source": [
        "\n",
        "\n",
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *\n",
        "\n",
        "**说明你的想法**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8c2868-b07c-44f2-a58c-0ffc49eedebf"
      },
      "source": [
        "'''\n",
        "# Please comment your code 评述你的代码 NLTK's Twitter_Sample dataset. \n",
        "# (Justify your decision) #[lab5]\n",
        "'''\n",
        "#[lab5]\n",
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#分集合;因为是给定的数据集都是数据+标签的形式，所以要把它分开\n",
        "\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))] #neg pos\n",
        "\n",
        "# 随机训练集 为了让模型训练的更好，因为给的训练集前面是前后分开的000111\n",
        "#看起来不加也行，加了准确率就提升了。。；没加之前是得根据batch_size 来调整训练集，要不然会出现训练模型时候，准确度0.5/1恒定的情况。 虽然loss在变，但是分类的情况却没有变。\n",
        "#[lab3]\n",
        "zipped = zip(train_data,train_label)   #上图说明 b 调用一次之后，再调用的 c 则为空。如果想要复用这个结果，需要保存到 list 里，即：\n",
        "Zipp = list(zipped) #如果直接 print(list(zipped))，则之后 zipped为空，因为已经用完一次迭代器了；如果想要复用这个结果，需要保存到 list 里\n",
        "from random import shuffle\n",
        "\n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "#做预处理 变小写 [lab5] 因为小写的话更容易匹配字典中的元素。就不会出现有些小写有些大写的情况，让数据集更加工整，也为了后续处理删词什么的方便。\n",
        "import pprint\n",
        "text_train = [s.lower() for s in train_data]\n",
        "text_test = [s.lower() for s in test_data]\n",
        "\n",
        "# 分词[lab5] 因为就是需要一个个词的来处理。通过一个或者多个词来判断整个句子的关系，那么首先要判断每个词的关系。\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "\n",
        "train_t=[]  #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "for s in text_train:\n",
        "  text_train = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "  train_t.append(text_train)  \n",
        "        #:(    :-(\n",
        "\n",
        "\n",
        "test_t=[]     #2000\n",
        "for w in text_test:\n",
        "  text_test = tknzr.tokenize(w)\n",
        "  test_t.append(text_test)\n",
        "\n",
        "# 删标点，这里是保留了一部分表情。但是后来发现，因为字典中没有表情，所以可以不保留表情[lab5]\n",
        "#theprincesszooz but i see what youre going at   \n",
        "#yes  subjective pain may not be real\n",
        "#  but that does not make it less painful\n",
        "'''\n",
        "还得改\n",
        "clean_doc2 = re.sub(r'[^\\w\\s]','',corpus[1])字符\n",
        "clean_doc2 = re.sub(r'\\d,'',corpus[1])数字\n",
        "'''\n",
        "import re\n",
        "def remove(x):\n",
        "  t = []\n",
        "  for i in range(len(x)):\n",
        "    t_sub = []        #是直接用空列表代替了\n",
        "    for j in range(len(x[i])):\n",
        "      if len(x[i][j])==0:\n",
        "        continue\n",
        "      if x[i][j] == \" \":\n",
        "        continue \n",
        "      else:\n",
        "        x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "        if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "          x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "        if len(x[i][j]) == 0:\n",
        "          continue \n",
        "        else:\n",
        "          t_sub.append(x[i][j]) \n",
        "    t.append(t_sub) \n",
        "\n",
        "  return t\n",
        "\n",
        "text_train = remove(train_t)\n",
        "text_test = remove(test_t)\n",
        "\n",
        "##停用词  删除那些可有可无不影响句意比如a;an;the 但是又数目非常多的词。\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "sww = sw.words()\n",
        "\n",
        "text_train_le=[]    #8000\n",
        "for tokens in text_train:\n",
        "    filtered_sentence1 = [w for w in tokens if not w in stop_words]\n",
        "    text_train_le.append(filtered_sentence1)\n",
        "\n",
        "\n",
        "text_test_le=[]   #2000\n",
        "for tokens in text_test:\n",
        "  filtered_sentence2 = [w for w in tokens if not w in stop_words]\n",
        "  text_test_le.append(filtered_sentence2)\n",
        "\n",
        "#去掉复数 stemmer 词根不需要这部； 如果用就需要在neg和pos需要stemmer操作 保留是因为更加便捷。\n",
        "#如果只保留原型lec…………那个，无法与现有给定字典一一匹配。\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "#text_train_le = [text_train_le[i] for i in range(len(text_train_le))]\n",
        "#singles = [stemmer.stem(plural) for plural in text_train_le[i] for i in range(len(text_train_le))]\n",
        "train_stem = []\n",
        "for i in range(len(text_train_le)):\n",
        "  singles1 = []\n",
        "  for plural in text_train_le[i]:\n",
        "    singles1.append(stemmer.stem(plural))\n",
        "  train_stem.append(singles1)\n",
        "\n",
        "#print(train_stem[:5])\n",
        "\n",
        "test_stem = []\n",
        "for i in range(len(text_test_le)):\n",
        "  singles2 = []\n",
        "  for plural in text_test_le[i]:\n",
        "    singles2.append(stemmer.stem(plural))\n",
        "  test_stem.append(singles2)\n",
        "\n",
        "#print(test_stem[:5])\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-"
      },
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *\n",
        "\n",
        "我到底用了哪个"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0f5b07-7b2f-4078-fc1b-0f68b00fe4e9"
      },
      "source": [
        "# Please comment your code\n",
        "#是调用的gensim模型[lab2]\n",
        "'''\n",
        "还有好几个模型没做对比尝试呢\n",
        "'''\n",
        "#已经在上面预处理完了，然后就是建立模型。\n",
        "from gensim.models import Word2Vec\n",
        "wv_cbow_model = Word2Vec(sentences= train_stem + test_stem, size=100, window=5, min_count=2, workers=2, sg=0)\n",
        "#训练模型\n",
        "#看看单词\n",
        "vocab = list(set(wv_cbow_model.wv.vocab.keys()))\n",
        "print('vocabulary:', len(vocab), vocab[:5])\n",
        "\n",
        "#找出某个词向量最相近的词集合。\n",
        "req_count = 100\n",
        "for key in wv_cbow_model.wv.similar_by_word('school', topn =100):   #'school'.decode(\"ISO-8859-1\")\n",
        "    if len(key[0])==5:  #最相近的五个字的词\n",
        "        req_count -= 1\n",
        "        print(key[0], key[1])\n",
        "        if req_count == 0:\n",
        "            break;\n",
        "\n",
        "#查看相似度。看两个词的相似度\n",
        "print(wv_cbow_model.wv.similarity('thing', 'hope'))\n",
        "print(wv_cbow_model.wv.similarity('phone', 'sleep'))\n",
        "\n",
        "#训练\n",
        "wv_cbow_model.save(\"cbow.model\")\n",
        "#训练 ； 存 取；训练。\n",
        "cbow = Word2Vec.load(\"./cbow.model\")  #只是训练的NLTK's Twitter_Sample dataset. cbow最原始的标记方法。\n",
        "#再训练epochs训练多少epoch ##end_alpha ==learning rate final;   total_examples : count of sentences.10000\n",
        "ep = 0\n",
        "for i in range(10):\n",
        "  cbow.train(sentences= train_stem + test_stem ,total_examples=wv_cbow_model.corpus_count, epochs=cbow.iter, compute_loss=True)\n",
        "  #举个例子，已经训练好的word2vec词表有[“我”,“爱”,“你”]三个词，给定一个新语料[“我”,“喜”,“欢”,“你”]，新语料只会调整原有词中的“我”“你”的词向量表示，而不会将“喜”“欢”添加到词库中。\n",
        "  ep += cbow.epochs\n",
        "  print('Epoch: %d, train loss: %.5f'%(ep, cbow.get_latest_training_loss()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary: 4327 ['sir', 'repost', 'tabinda_samar', 'chat', 'ahead']\n",
            "thing 0.9998672008514404\n",
            "watch 0.9998614192008972\n",
            "gonna 0.9998530149459839\n",
            "still 0.9998481273651123\n",
            "first 0.999843955039978\n",
            "right 0.9998433589935303\n",
            "peopl 0.9998365640640259\n",
            "anoth 0.9998319149017334\n",
            "there 0.9998281002044678\n",
            "littl 0.9998272657394409\n",
            "never 0.9998247623443604\n",
            "music 0.9998242259025574\n",
            "video 0.9998210668563843\n",
            "wanna 0.999817967414856\n",
            "could 0.999816358089447\n",
            "alway 0.9998159408569336\n",
            "think 0.9998087286949158\n",
            "today 0.9998081922531128\n",
            "phone 0.9998078346252441\n",
            "0.99989986\n",
            "0.99976516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5, train loss: 350616.84375\n",
            "Epoch: 10, train loss: 334654.25000\n",
            "Epoch: 15, train loss: 320123.78125\n",
            "Epoch: 20, train loss: 306175.90625\n",
            "Epoch: 25, train loss: 284001.65625\n",
            "Epoch: 30, train loss: 278337.12500\n",
            "Epoch: 35, train loss: 273414.75000\n",
            "Epoch: 40, train loss: 249550.78125\n",
            "Epoch: 45, train loss: 227826.43750\n",
            "Epoch: 50, train loss: 221944.10938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Lexicon Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Lexicon-based Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbLnN-3GlI1"
      },
      "source": [
        "*[Optional] You are required to describe why you would like to use more than one-dimensional embedding.*\n",
        "\n",
        "我认为是为了包含更多的信息也就是特征，更加准确的去得到你输入的和你想要结果之间的某种关系。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CUCL1cGlI2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "4f9eafea-5b5a-48f3-cb37-ba014e118a88"
      },
      "source": [
        "# Please comment your code\n",
        "#否定词\"ISO-8859-1\"==UTF-8\n",
        "'''\n",
        "这里需要具体说明把文件丢到哪里\n",
        "'''\n",
        "#核心想法，我去看了看这个文件，因为有一些文本介绍。我找到了每个文件的第一个词，然后把这些词重新取出来放到一个list中。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #必须要挂载\n",
        "not_words = [w.strip() for w in open('/content/drive/MyDrive/NLP/A1/E/opinion-lexicon-English/negative-words.txt', 'r', encoding=\"ISO-8859-1\").readlines()]\n",
        "#处理文件\n",
        "j = -1\n",
        "for i in not_words:\n",
        "  j +=1\n",
        "  if i == '2-faced':\n",
        "    break\n",
        "neg = not_words[j:]\n",
        "\n",
        "#处理文件\n",
        "sure_words = [w.strip() for w in open('/content/drive/MyDrive/NLP/A1/E/opinion-lexicon-English/positive-words.txt', 'r', encoding=\"ISO-8859-1\").readlines()]\n",
        "\n",
        "j = -1\n",
        "for i in sure_words:\n",
        "  j +=1\n",
        "  if i == 'a+':\n",
        "    break\n",
        "\n",
        "pos = sure_words[j:]\n",
        "\n",
        "#neg pos 词表\n",
        "#要对他们做stem因为训练集做了stem\n",
        "\n",
        "'''\n",
        "前面要是后来测试发现不需要这里那就不用了。\n",
        "或者存个模型。\n",
        "'''\n",
        "#[lab5]\n",
        "#也要对他做stem #太久了。简直了 至少10-20分钟，太久了。 #该跑还是要跑。\n",
        "\n",
        "neg_stem = []\n",
        "for i in range(len(neg)):\n",
        "  singles3 = []\n",
        "  for plural in neg:\n",
        "    singles3.append(stemmer.stem(plural))\n",
        "  neg_stem.append(singles3)\n",
        "\n",
        "print(neg_stem[:5])\n",
        "\n",
        "\n",
        "pos_stem = []\n",
        "for i in range(len(pos)):\n",
        "  singles4 = []\n",
        "  for plural in pos:\n",
        "    singles4.append(stemmer.stem(plural))\n",
        "  pos_stem.append(singles4)\n",
        "\n",
        "print(pos_stem[:5])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nneg_stem = []\\nfor i in range(len(neg)):\\n  singles3 = []\\n  for plural in neg:\\n    singles3.append(stemmer.stem(plural))\\n  neg_stem.append(singles3)\\n\\nprint(neg_stem[:5])\\n\\n\\npos_stem = []\\nfor i in range(len(pos)):\\n  singles4 = []\\n  for plural in pos:\\n    singles4.append(stemmer.stem(plural))\\n  pos_stem.append(singles4)\\n\\nprint(pos_stem[:5])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEN6r2DGydLN"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF583nnfujzI"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "zz5pDsHF_-wZ",
        "outputId": "bde20a4a-9506-48d9-eb27-b5a1c25a2f87"
      },
      "source": [
        "#把每次训练的给存了起来。 \n",
        "#neg_stem.save(\"neg_stem\")  #AttributeError: 'list' object has no attribute 'save'\n",
        "#pos_stem.save(\"pos_stem\")\n",
        "#\n",
        "'''\n",
        "1.我们先将需要保存的数据解析好，保存成固定的数据类型（保存成列表，元组，字典都可以，根据具体场景来选择）\n",
        "\n",
        "2.我们将保存数据到csv文件的代码封装成一个函数，方便重用。步骤主要分为三步：打开文件，写入数据，关闭文件。其中，写入数据时记得先写入表头（我们使用excel打开时需要表头）再写入表格中的数据，数据要以一个列表的形式传递给writerows()。\n",
        "\n",
        "运行结果：\n",
        "\n",
        "运行以上代码后，会在当前目录下创建一个csv_file.csv的文件，并写入csv_data的数据，可以使用excel打开文件查看。如下图。\n",
        "1.可以通过with上下文管理的方式打开csv文件，如果想在with的代码块外对读出来的数据进行处理，则可以使用open()打开，再使用close()关闭。\n",
        "\n",
        "2.csv通过csv.reader()来打开csv文件，返回的是一个列表格式的迭代器，可以通过next()方法获取其中的元素，也可以使用for循环依次取出所有元素。\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1.我们先将需要保存的数据解析好，保存成固定的数据类型（保存成列表，元组，字典都可以，根据具体场景来选择）\\n\\n2.我们将保存数据到csv文件的代码封装成一个函数，方便重用。步骤主要分为三步：打开文件，写入数据，关闭文件。其中，写入数据时记得先写入表头（我们使用excel打开时需要表头）再写入表格中的数据，数据要以一个列表的形式传递给writerows()。\\n\\n运行结果：\\n\\n运行以上代码后，会在当前目录下创建一个csv_file.csv的文件，并写入csv_data的数据，可以使用excel打开文件查看。如下图。\\n1.可以通过with上下文管理的方式打开csv文件，如果想在with的代码块外对读出来的数据进行处理，则可以使用open()打开，再使用close()关闭。\\n\\n2.csv通过csv.reader()来打开csv文件，返回的是一个列表格式的迭代器，可以通过next()方法获取其中的元素，也可以使用for循环依次取出所有元素。\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW9nsl931n9e"
      },
      "source": [
        "#把列表存进去\n",
        "'''\n",
        "import csv\n",
        "neg_name = 'neg.csv'\n",
        "pos_name = 'pos.csv'\n",
        "def save_csv(target_list, output_file_name):\n",
        "    \"\"\"\n",
        "    将数据写入csv文件\n",
        "    \"\"\"\n",
        "    if not output_file_name.endswith('.csv'):\n",
        "        output_file_name += '.csv'\n",
        "    neg = open(output_file_name, \"w\", newline=\"\")\n",
        "    key_data = target_list[0]\n",
        "    value_data = [target for target in target_list]\n",
        "    csv_writer = csv.writer(neg)\n",
        "    csv_writer.writerow(key_data)\n",
        "    csv_writer.writerows(value_data)\n",
        "    neg.close()\n",
        " \n",
        " \n",
        "save_csv(neg_stem, neg_name)\n",
        "save_csv(pos_stem, pos_name)\n",
        "\n",
        "input_file_name1 = 'neg.csv'\n",
        "input_file_name2 = 'pos.csv'\n",
        "\n",
        "with open(input_file_name1, 'r', encoding='ISO-8859-1') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    neg = next(csv_reader)    #neg == neg_stem\n",
        "\n",
        "with open(input_file_name2, 'r', encoding='ISO-8859-1') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    pos = next(csv_reader)    #  pos ==pos_stem\n",
        "\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c438ph4T6OtC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daV6yc2dz9Gz"
      },
      "source": [
        "'''\n",
        "\n",
        "import csv\n",
        "import codecs\n",
        "\n",
        "def data_write_csv(file_name, datas):#file_name为写入CSV文件的路径，datas为要写入数据列表\n",
        "          file_csv = codecs.open(file_name,'w+','utf-8')#追加\n",
        "          writer = csv.writer(file_csv, delimiter=' ', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
        "          for data in datas:\n",
        "              writer.writerow(data)\n",
        "          print(\"保存文件成功，处理结束\")\n",
        "data_write_csv()\n",
        "'''\n",
        "\n",
        "\n",
        " \n",
        "'''\n",
        "csv_data = (\n",
        "    (1, 2, 3, 4, 5, 6),\n",
        "    ('a', 'b', 'c', 'd', 'e', 'f'),\n",
        "    ('p', 'y', 't', 'h', 'o', 'n')\n",
        ")\n",
        "'''\n",
        "\n",
        "import csv\n",
        "neg_name = 'neg.csv'\n",
        " \n",
        " \n",
        "def save_csv(target_list, output_file_name):\n",
        "    \"\"\"\n",
        "    将数据写入csv文件\n",
        "    \"\"\"\n",
        "    if not output_file_name.endswith('.csv'):\n",
        "        output_file_name += '.csv'\n",
        "    neg = open(output_file_name, \"w\", newline=\"\")\n",
        "    key_data = target_list[0]\n",
        "    value_data = [target for target in target_list]\n",
        "    csv_writer = csv.writer(neg)\n",
        "    csv_writer.writerow(key_data)\n",
        "    csv_writer.writerows(value_data)\n",
        "    neg.close()\n",
        " \n",
        " \n",
        "save_csv(neg_stem, neg_name)\n",
        "\n",
        "\n",
        "input_file_name = 'neg.csv'\n",
        " \n",
        "\n",
        "with open(input_file_name, 'r', encoding='ISO-8859-1') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    neg = next(csv_reader)\n",
        "    # csv_reader对象，是一个列表的格式\n",
        "    #print(csv_reader)\n",
        "    # csv_reader对象的一个迭代器，可以通过next()取出其中的元素\n",
        "    #print(next(csv_reader))\n",
        "    # 也可以通过for循环取出所有元素\n",
        "    #for line in csv_reader:\n",
        "        #print(''.join(line))\n",
        "        \n",
        "\n",
        "''' \n",
        "def read_csv(input_file_name):\n",
        "    \"\"\"\n",
        "    读取csv文件数据\n",
        "    \"\"\"\n",
        "    with open(input_file_name, 'r', encoding='ISO-8859-1') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        # csv_reader对象，是一个列表的格式\n",
        "        print(csv_reader)\n",
        "        # csv_reader对象的一个迭代器，可以通过next()取出其中的元素\n",
        "        print(next(csv_reader))\n",
        "        # 也可以通过for循环取出所有元素\n",
        "        #for line in csv_reader:\n",
        "            #print(''.join(line))\n",
        " \n",
        " \n",
        "neg = read_csv(neg_name)\n",
        "'''\n",
        "\n",
        "#https://blog.csdn.net/weixin_43790276/article/details/90572596?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161941924416780269863927%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161941924416780269863927&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-90572596.first_rank_v2_pc_rank_v29&utm_term=csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34nQ8HTW5aRT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrcNBGlGAM2u"
      },
      "source": [
        "'''\n",
        "import csv\n",
        "neg_file_name = 'neg_stem.csv'\n",
        "\n",
        "\n",
        "def save_csv(target_list, output_file_name):\n",
        "  #将数据写入CSV\n",
        "  if not output_file_name.endswith('.csv'):\n",
        "    output_file_name +='.csv'\n",
        "  csv_file = open(output_file_name, \"w\",newline=\"\") #来控制空行数。\n",
        "  key_data = target_list[0]\n",
        "  value_data = [target for target in target_list]\n",
        "  csv_writer = csv.writer(csv_file) #创建一个csv的写入器\n",
        "  csv_writer.writerow(key_data) #写入标签\n",
        "  csv_writer.writerows(value_data)  #写入样本数据\n",
        "  csv_file.close()\n",
        "\n",
        "save_csv(neg_stem, neg_file_name)\n",
        "save_csv(pos_stem, pos_file_name)\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2CaUJ_YCnPU"
      },
      "source": [
        "#字典的embedding ； 增加一个维度的 \n",
        "#只需要先让他们对应起来。您要检查每个单词是在正还是负词典中not_exist（0），negative（1）和positive（2）\n",
        "#一维句子，二维单词\n",
        "#是要吧一句话变成【012组成的形式】\n",
        "#现在就是一一对应的形式了。一句话;这个是train_stem【 as,ada,daff】 match_embedding【120】同一个索引在两个列表中 \n",
        "def match(text):\n",
        "  sum_match= [] #train_match\n",
        "  for i in range(len(text)):  #train_stem\n",
        "    match = []\n",
        "    for j in range(len(text[i])):\n",
        "      if text[i][j] in neg_stem[0]:\n",
        "        match.append(1)   #neg 1\n",
        "      elif text[i][j] in pos_stem[0]:\n",
        "        match.append(2) #pos 2\n",
        "      else:\n",
        "        match.append(0)\n",
        "    sum_match.append(match)\n",
        "  #print(sun_match[:5])\n",
        "  #print(text[:5]) \n",
        "  return sum_match\n",
        "train_match = match(train_stem)\n",
        "test_match = match(test_stem)\n",
        "#只有stem好用\n",
        "print(train_match[:5])\n",
        "print(test_match[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd"
      },
      "source": [
        "## 2.3. Bi-directional RNN Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Lexicon Embedding\n",
        "\n",
        "把词嵌入和字典嵌入连接起来。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2"
      },
      "source": [
        "# Please comment your code\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R"
      },
      "source": [
        "### 2.3.3. Train Sequence Model\n",
        "\n",
        "Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 3 - Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLBzHObsvvM"
      },
      "source": [
        "## 3.1. Word Embedding Evaluation\n",
        "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSIUsb7qtQEf"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCrcXwcGsuuo"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 3.2. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCF0bwTtRS0"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 3.3. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.* Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYzrA_s2tTaz"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVmx4E52dXS"
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
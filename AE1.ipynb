{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh0PSrd+0Wuprt8ZGIkHpn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/AE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdKeKaytXXFC"
      },
      "source": [
        "AE1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8db7b9d-07ce-457a-c9ca-136e9528a46c"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data))) #8000\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data))) #2000\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n",
        "#LABEL: neg / SENTENCE: hopeless for tmr :("
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avOXtD97ePN6"
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe"
      },
      "source": [
        "\n",
        "\n",
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *\n",
        "è¯·æ‚¨è¯´æ˜é‡‡ç”¨äº†å“ªäº›æ•°æ®é¢„å¤„ç†æŠ€æœ¯ï¼Œå¹¶è¯´æ˜å†³å®šçš„ç†ç”±ã€‚*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMRLNNzIX0fk"
      },
      "source": [
        "\n",
        "#é¢„å¤„ç†ï¼š 2åˆ†ï¼š\n",
        "'''\n",
        "æ•°æ®é›†ï¼š NLTK's Twitter_Sample dataset ã€via APIsã€‘å¯ä»¥æŸ¥çœ‹ç»†èŠ‚\n",
        "\n",
        "è®­ç»ƒå’Œæµ‹è¯•éƒ½è¢«æä¾›äº†(testing_data.pkl, training_data.pkl) å¯ä»¥ç”¨A1 template ipynb \n",
        "ï¼Œå¯ä»¥ä½¿ç”¨ä½œä¸š1æ¨¡æ¿(template)ipynbä¸­æä¾›çš„ä»£ç ä»Google Driveä¸­ä¸‹è½½ã€‚\n",
        "\n",
        "[è¦æ±‚]\n",
        "  é¢„å¤„ç†è®­ç»ƒé›†ç”¨lab5çš„å‡ ä¸ªæŠ€æœ¯çš„åˆé›†\n",
        "  (e.g. tokenisation, removing numbers, converting to lowercase, removing stop words, stemming, etc.).\n",
        "  è¦è¯æ˜ä¸ºä»€ä¹ˆè¦ç”¨ã€‚#æ‚¨åº”è¯¥è¯´æ˜æ‚¨åº”ç”¨ç‰¹å®šé¢„å¤„ç†æŠ€æœ¯çš„åŸå› ï¼ˆè¯´æ˜æ‚¨çš„å†³å®šï¼‰ã€‚\n",
        "  ã€çœ‹èµ·æ¥æœ‰äº›æ˜¯ä¸èƒ½ç”¨çš„è¦æµ‹è¯•ã€‘\n",
        "  po',\n",
        "  'ff',\n",
        "  'follow',\n",
        "  'let',\n",
        "  'tri',\n",
        "  'keepitloc',\n",
        "  'po',\n",
        "  'teen_emma',\n",
        "  'br1stler',\n",
        "  'jeweleyegoddess',\n",
        "  'caratoyn',\n",
        "  'caraa\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWgfDdBdyhu"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import pprint\n",
        "from nltk.stem.porter import *\n",
        "#lemmatisation\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "sww = sw.words()\n",
        "sww.extend([\"'\", '\"'])\n",
        "\n",
        "#tokenized_docs=[]\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "clean_doc = re.sub(r'[^\\w\\s]','', str(training_data))   #removing numbers\n",
        "tokenized_sentence = sent_tokenize(clean_doc.lower())   #è¿™é‡Œæ˜å¤©lab5å†çœ‹çœ‹ converting to lowercase\n",
        "lower_case = word_tokenize(clean_doc.lower())   #tokenisation\n",
        "stopword_removal = [w for w in lower_case if not w in sww]    #removing stop words\n",
        "#singles = [stemmer.stem(plural) for plural in plurals]#stemming\n",
        "singles = [stemmer.stem(plural) for plural in stopword_removal] #æ— å¤æ•°\n",
        "lemma_sentence = [lemmatizer.lemmatize(w) for w in singles ]\n",
        "#tokenized_docs.append(lemma_sentence)\n",
        "pprint.pprint(lemma_sentence)\n",
        "\n",
        "#pprint.pprint(tokenized_docs) #58613\n",
        "#æ‰€ä»¥åˆ°åº•è¦ä¸è¦åŠ è¿™ä¸ªåˆ—è¡¨ï¼ŒåŠ å®ƒæœ‰å•¥å­ç”¨\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEGG_861g4pL",
        "outputId": "50c9ea2c-355e-47d9-840c-5016824c3697"
      },
      "source": [
        "len(lemma_sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12"
      },
      "source": [
        "# Please comment your code è¯·è¯„è¿°ä½ çš„ä»£ç \n",
        "def preprocess(corpus):\n",
        "  sww = sw.words()\n",
        "  sww.extend([\"'\", '\"'])\n",
        "\n",
        "  #tokenized_docs=[]\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  clean_doc = re.sub(r'[^\\w\\s]','', str(corpus))   #removing numbers\n",
        "  tokenized_sentence = sent_tokenize(clean_doc.lower())   #è¿™é‡Œæ˜å¤©lab5å†çœ‹çœ‹ converting to lowercase\n",
        "  lower_case = word_tokenize(clean_doc.lower())   #tokenisation\n",
        "  stopword_removal = [w for w in lower_case if not w in sww]    #removing stop words\n",
        "  #singles = [stemmer.stem(plural) for plural in plurals]#stemming\n",
        "  singles = [stemmer.stem(plural) for plural in stopword_removal] #æ— å¤æ•°\n",
        "  lemma_sentence = [lemmatizer.lemmatize(w) for w in singles ]\n",
        "  #tokenized_docs.append(lemma_sentence)\n",
        "  #pprint.pprint(lemma_sentence)\n",
        "  return lemma_sentence #tokenized_docs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr0wnTubjHdk"
      },
      "source": [
        "#æ¨¡å—å®ç° model implementation 7åˆ†\n",
        "'''\n",
        "ã€è¦åšçš„ã€‘\n",
        "  1ï¼Œword embedding module, 2ï¼ŒLexicon Embedding module + 3ï¼ŒBi-directional RNN sequence model\n",
        "\n",
        "  [ä»lab2,4,5éšä¾¿é€‰å‚æ•°]embeddings_dimension ; learning_rate epochs;\n",
        "  [lab5 æœ‰æ¨¡å‹æ„å»º]\n",
        "\n",
        "\n",
        "# word embedding  2åˆ† [lab2]\n",
        "  åˆ›å»ºword embedding model(è¡¨ç°word vectors eg: word2vec-CBOW, word2vec-Skip gram, fastText + Glove[lab2])\n",
        "  input embedding of your sequence model\n",
        "  [lab3,4]ä½¿ç”¨äº†one-hot vectors ä½œä¸ºinputs ç»™é•¿åºåˆ—æ¨¡å‹\n",
        "  ã€è¦æ±‚åšçš„ã€‘\n",
        "    1.æ•°æ®çš„é¢„å¤„ç†ï¼ˆword embeddings[lab2;è¿˜è¦åšè§£é‡Šè¯´æ˜]ï¼‰ NLTK Twitter dataset ( section 1 æä¾›äº†) or/and ä»»ä½•æ•°æ®é›†ï¼ˆTED talk , Google News)\n",
        "    2.åˆ›å»ºè®­ç»ƒæ¨¡å‹ï¼ˆfor word embeddings) åˆ›å»ºè®­ç»ƒæ¨¡å‹ï¼›éœ€è¦è¯´æ¸…æ¥šè¶…å‚æ•°ã€lab2ï¼ˆdimension of embeddings; window size, learning rate,etc.ã€‘ \n",
        "        note ä»»ä½•word embeddings model[lab2] (word2vec-CBOW, word2vec-Skip gram, fasttext, glove)ã€è¯æ˜ã€‘\n",
        "    3.è®­ç»ƒæ¨¡å‹ï¼š train model\n",
        "\n",
        "#Lexicon embedding 2åˆ† ï¼ˆè¯å…¸åµŒå…¥ï¼‰\n",
        "  æ¯ä¸ªå•è¯çš„ç§¯ææˆ–è€…æ¶ˆæçš„lexiconã€ä»æ–‡ä»¶å¤¹ä¸‹è½½ï¼šopinion Lexicaonã€‘(2006 positive and 4783 negative words)\n",
        "  æ¯ä¸ªå•è¯è¦è¢«æ”¹æˆï¼š one-dimensionalä¸€ç»´åˆ†ç±»åµŒå…¥ï¼Œä¸‰ç±»ã€‚ã€eg:[not_exist(0), negative(1), positive(2);è¿™ä¸ª012ç±»åˆ« ä¼šåœ¨[2.3 çš„Bi-directional RNN æ¨¡å‹ä¸­ç”¨åˆ°]]\n",
        "  ã€å¦‚æœä½ æƒ³ç”¨è¶…è¿‡ä¸€ç»´ï¼Œæˆ–è€…ä¸ä½¿ç”¨åˆ†ç±»åµŒå…¥ï¼ˆcategoricalï¼‰ è¯æ˜è§‚ç‚¹ã€‘\n",
        "\n",
        "# Bi-directional RNN Sequence Model 3åˆ†\n",
        "Many-to-One (N to 1) å»ºä¸ªn to 1 sequence model ä¸ºäº†æ¢æµ‹æ„Ÿæƒ…ï¼ˆdetect sentiment/emotionï¼‰ä½ çš„æ¨¡å‹åº”è¯¥æ˜¯è¯„ä»·ä¸­é€‰å‡ºçš„æœ€ä½³æ¨¡å‹ï¼ˆå°†åœ¨ç¬¬3èŠ‚è¯„ä»·ã€evaluationã€‘ä¸­è®¨è®ºï¼‰ã€‚\n",
        "  ã€åšçš„ã€‘\n",
        "    1.apply/import word + lexicon embedding as input: ä½ è¦æŠŠè®­ç»ƒå¥½çš„è¯åµŒå…¥å’Œè¯åº“åµŒå…¥åˆå¹¶èµ·æ¥ï¼Œå¹¶åº”ç”¨åˆ°åºåˆ—æ¨¡å‹ä¸Š    ##ä¸å¤ªç†è§£\n",
        "    2.1æ„å»ºè®­ç»ƒSequence model: æ„å»ºBi-directional RNN-based(Bi-RNN / Bi-LSTM / Bi-GRU) \n",
        "    2.2Many to One (N to 1) sequence model (N: word, One: sentiment- positive or negative)\n",
        "    3.æè¿°è¶…å‚ã€è¯æ˜ã€‘ã€lab4,5ã€‘[the number of epochs, learning rate â€¦â€¦]\n",
        "    4.Train model  å±•ç¤ºï¼štraining loss + number of epochs[lab4,lab5]\n",
        "\n",
        "    ã€ä¸å±•ç¤ºæ²¡åˆ†ã€‘\n",
        "'''\n",
        "'''\n",
        "ã€è¦åšçš„ã€‘\n",
        "  1ï¼Œword embedding module, 2ï¼ŒLexicon Embedding module + 3ï¼ŒBi-directional RNN sequence model\n",
        "\n",
        "  [ä»lab2,4,5éšä¾¿é€‰å‚æ•°]embeddings_dimension ; learning_rate epochs;\n",
        "  [lab5 æœ‰æ¨¡å‹æ„å»º]\n",
        "\n",
        "\n",
        "# word embedding  2åˆ† [lab2]\n",
        "  åˆ›å»ºword embedding model(è¡¨ç°word vectors eg: word2vec-CBOW, word2vec-Skip gram, fastText + Glove[lab2])\n",
        "  input embedding of your sequence model\n",
        "  [lab3,4]ä½¿ç”¨äº†one-hot vectors ä½œä¸ºinputs ç»™é•¿åºåˆ—æ¨¡å‹\n",
        "  ã€è¦æ±‚åšçš„ã€‘\n",
        "    1.æ•°æ®çš„é¢„å¤„ç†ï¼ˆword embeddings[lab2;è¿˜è¦åšè§£é‡Šè¯´æ˜]ï¼‰ NLTK Twitter dataset ( section 1 æä¾›äº†) or/and ä»»ä½•æ•°æ®é›†ï¼ˆTED talk , Google News)\n",
        "    2.åˆ›å»ºè®­ç»ƒæ¨¡å‹ï¼ˆfor word embeddings) åˆ›å»ºè®­ç»ƒæ¨¡å‹ï¼›éœ€è¦è¯´æ¸…æ¥šè¶…å‚æ•°ã€lab2ï¼ˆdimension of embeddings; window size, learning rate,etc.ã€‘ \n",
        "        note ä»»ä½•word embeddings model[lab2] (word2vec-CBOW, word2vec-Skip gram, fasttext, glove)ã€è¯æ˜ã€‘\n",
        "    3.è®­ç»ƒæ¨¡å‹ï¼š train model\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-"
      },
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *\n",
        "*è¦æ±‚æ‚¨è¯´æ˜é‡‡ç”¨çš„æ˜¯å“ªç§æ¨¡å¼ï¼ˆå³Word2Vecä¸CBOWã€FastTextä¸SkipGramç­‰ï¼‰ï¼Œå¹¶è¯´æ˜æ‚¨å†³å®šçš„ç†ç”±*ã€‚ã€ä¹Ÿå°±æ˜¯ä¸‰ç§éƒ½è¦åšä¸€åšã€‘"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings å­—è¯åµŒå…¥çš„æ•°æ®é¢„å¤„ç†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*\n",
        "æ‚¨éœ€è¦è¯´æ˜ä½¿ç”¨äº†å“ªäº›é¢„å¤„ç†æŠ€æœ¯ï¼Œå¹¶è¯´æ˜æ‚¨çš„å†³å®šçš„ç†ç”±ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model. å»ºç«‹å•è¯åµŒå…¥æ¨¡å‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*æ‚¨éœ€è¦è¯´æ˜è¶…å‚æ•°æ˜¯å¦‚ä½•å†³å®šçš„ï¼Œå¹¶è¯´æ˜å†³å®šçš„ç†ç”±ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfOocivpX7jz"
      },
      "source": [
        "#Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcAXL_Z3Y6OM",
        "outputId": "b3d4935d-c30b-4d46-883d-c4f0ab26144f"
      },
      "source": [
        "testing_data[0]#('neg', '@AndreaMarySmith very helpful .... Or will be once I stop crying :(')\n",
        "'''\n",
        "éƒ½æ˜¯æ ‡ç­¾+å¥å­\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', '@AndreaMarySmith very helpful .... Or will be once I stop crying :(')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfTnAoI4YtQz",
        "outputId": "09489ebb-97e1-4216-ed09-4736f25df7af"
      },
      "source": [
        "training_data[0]#('neg', 'hopeless for tmr :(')\n",
        "training_data[1]#('neg',\"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\")\n",
        "training_data[2]#('neg', '@Hegelbon That heart sliding into the waste basket. :(')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', '@Hegelbon That heart sliding into the waste basket. :(')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic3G1zAZYnen",
        "outputId": "3dd7c37c-14e3-497c-ff67-ef532489e6c0"
      },
      "source": [
        "len(testing_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTXrf6OYjGn"
      },
      "source": [
        "testing_data  #2000\n",
        "'''\n",
        " \"@NUFC_Vine he's gona miss Sheff Utd/York probably waiting for a visa which leaves gim with 1 friendly. Bet he starts season on the bench :(\"),\n",
        " ('neg', \"@ranaPTX_ BUT WHO WILL LEAD US WHEN YOU'RE GONE :((\"),\n",
        " ('neg', '@LottyStorer @charliee_catlin ğŸ˜‚ğŸ˜©ğŸ˜© what a mess :('),\n",
        " ('neg', '@twinitisha irony is its more harmful .. :('),\n",
        " ('neg', '@DollyyDaydreams I miss you so much more :( xxx'),\n",
        " ('neg', 'no motivation :('),\n",
        " ('neg',\n",
        "  '@LiLGeekette but that is the point. Because it is YOLO I sort of die when lactose is in me... Ugh like 3 months pregnant bloating :('),\n",
        " (\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w17KNAbyYGW8",
        "outputId": "f412e2eb-4487-4c5f-e38e-037f73a958a9"
      },
      "source": [
        "len(training_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "b76arcTaX8st",
        "outputId": "dcecefb2-511f-42b6-860a-b3c297dd484b"
      },
      "source": [
        "training_data #8000\n",
        "'''\n",
        " ('neg', \"@rupexo @LordOfTheMics fuckkk it's a Wednesday :(\"),\n",
        " ('neg',\n",
        "  '@PetiteMistress DO IT! I want to start one for making small games, but I feel like I need to get a jump start before asking for support :('),\n",
        " ('neg', 'Justin where are you ? :( @justinbieber'),\n",
        " ('neg', \":( :( I'm gonna cry\"),\n",
        " ('neg',\n",
        "  \"@IanHallard @ArtsTheatreLDN I'm really excited but also really sad because once again I won't be able to see it :(\"),\n",
        " ('neg', \"I can't finish my Sanum today! :(\\n#LLAOLLAO #Dessert but too full\"),\n",
        " ('neg', '@jxstkatie @TheSvante i want foood :(('),\n",
        " ('neg', 'Last day :('),\n",
        " ('neg', '#UberIceCream was super! But we didnt get the glares :( @Uber_Pune'),\n",
        " ('neg', \"@ffyeahh i don't know how to do vines :(\"),\n",
        " ('neg',\n",
        "  \"@selenagomez you was Tweetin earlier this morning &amp; i wasn't here :( sad mood all the day :(\"),\n",
        " ('n\n",
        " '''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n (\\'neg\\', \"@rupexo @LordOfTheMics fuckkk it\\'s a Wednesday :(\"),\\n (\\'neg\\',\\n  \\'@PetiteMistress DO IT! I want to start one for making small games, but I feel like I need to get a jump start before asking for support :(\\'),\\n (\\'neg\\', \\'Justin where are you ? :( @justinbieber\\'),\\n (\\'neg\\', \":( :( I\\'m gonna cry\"),\\n (\\'neg\\',\\n  \"@IanHallard @ArtsTheatreLDN I\\'m really excited but also really sad because once again I won\\'t be able to see it :(\"),\\n (\\'neg\\', \"I can\\'t finish my Sanum today! :(\\n#LLAOLLAO #Dessert but too full\"),\\n (\\'neg\\', \\'@jxstkatie @TheSvante i want foood :((\\'),\\n (\\'neg\\', \\'Last day :(\\'),\\n (\\'neg\\', \\'#UberIceCream was super! But we didnt get the glares :( @Uber_Pune\\'),\\n (\\'neg\\', \"@ffyeahh i don\\'t know how to do vines :(\"),\\n (\\'neg\\',\\n  \"@selenagomez you was Tweetin earlier this morning &amp; i wasn\\'t here :( sad mood all the day :(\"),\\n (\\'n\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D7sHyZeYC0w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
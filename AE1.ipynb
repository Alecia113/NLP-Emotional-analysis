{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh0PSrd+0Wuprt8ZGIkHpn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/AE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdKeKaytXXFC"
      },
      "source": [
        "AE1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8db7b9d-07ce-457a-c9ca-136e9528a46c"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data))) #8000\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data))) #2000\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n",
        "#LABEL: neg / SENTENCE: hopeless for tmr :("
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avOXtD97ePN6"
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe"
      },
      "source": [
        "\n",
        "\n",
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *\n",
        "请您说明采用了哪些数据预处理技术，并说明决定的理由。*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMRLNNzIX0fk"
      },
      "source": [
        "\n",
        "#预处理： 2分：\n",
        "'''\n",
        "数据集： NLTK's Twitter_Sample dataset 【via APIs】可以查看细节\n",
        "\n",
        "训练和测试都被提供了(testing_data.pkl, training_data.pkl) 可以用A1 template ipynb \n",
        "，可以使用作业1模板(template)ipynb中提供的代码从Google Drive中下载。\n",
        "\n",
        "[要求]\n",
        "  预处理训练集用lab5的几个技术的合集\n",
        "  (e.g. tokenisation, removing numbers, converting to lowercase, removing stop words, stemming, etc.).\n",
        "  要证明为什么要用。#您应该说明您应用特定预处理技术的原因（说明您的决定）。\n",
        "  【看起来有些是不能用的要测试】\n",
        "  po',\n",
        "  'ff',\n",
        "  'follow',\n",
        "  'let',\n",
        "  'tri',\n",
        "  'keepitloc',\n",
        "  'po',\n",
        "  'teen_emma',\n",
        "  'br1stler',\n",
        "  'jeweleyegoddess',\n",
        "  'caratoyn',\n",
        "  'caraa\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWgfDdBdyhu"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import pprint\n",
        "from nltk.stem.porter import *\n",
        "#lemmatisation\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "sww = sw.words()\n",
        "sww.extend([\"'\", '\"'])\n",
        "\n",
        "#tokenized_docs=[]\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "clean_doc = re.sub(r'[^\\w\\s]','', str(training_data))   #removing numbers\n",
        "tokenized_sentence = sent_tokenize(clean_doc.lower())   #这里明天lab5再看看 converting to lowercase\n",
        "lower_case = word_tokenize(clean_doc.lower())   #tokenisation\n",
        "stopword_removal = [w for w in lower_case if not w in sww]    #removing stop words\n",
        "#singles = [stemmer.stem(plural) for plural in plurals]#stemming\n",
        "singles = [stemmer.stem(plural) for plural in stopword_removal] #无复数\n",
        "lemma_sentence = [lemmatizer.lemmatize(w) for w in singles ]\n",
        "#tokenized_docs.append(lemma_sentence)\n",
        "pprint.pprint(lemma_sentence)\n",
        "\n",
        "#pprint.pprint(tokenized_docs) #58613\n",
        "#所以到底要不要加这个列表，加它有啥子用\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEGG_861g4pL",
        "outputId": "50c9ea2c-355e-47d9-840c-5016824c3697"
      },
      "source": [
        "len(lemma_sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12"
      },
      "source": [
        "# Please comment your code 请评述你的代码\n",
        "def preprocess(corpus):\n",
        "  sww = sw.words()\n",
        "  sww.extend([\"'\", '\"'])\n",
        "\n",
        "  #tokenized_docs=[]\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  clean_doc = re.sub(r'[^\\w\\s]','', str(corpus))   #removing numbers\n",
        "  tokenized_sentence = sent_tokenize(clean_doc.lower())   #这里明天lab5再看看 converting to lowercase\n",
        "  lower_case = word_tokenize(clean_doc.lower())   #tokenisation\n",
        "  stopword_removal = [w for w in lower_case if not w in sww]    #removing stop words\n",
        "  #singles = [stemmer.stem(plural) for plural in plurals]#stemming\n",
        "  singles = [stemmer.stem(plural) for plural in stopword_removal] #无复数\n",
        "  lemma_sentence = [lemmatizer.lemmatize(w) for w in singles ]\n",
        "  #tokenized_docs.append(lemma_sentence)\n",
        "  #pprint.pprint(lemma_sentence)\n",
        "  return lemma_sentence #tokenized_docs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr0wnTubjHdk"
      },
      "source": [
        "#模块实现 model implementation 7分\n",
        "'''\n",
        "【要做的】\n",
        "  1，word embedding module, 2，Lexicon Embedding module + 3，Bi-directional RNN sequence model\n",
        "\n",
        "  [从lab2,4,5随便选参数]embeddings_dimension ; learning_rate epochs;\n",
        "  [lab5 有模型构建]\n",
        "\n",
        "\n",
        "# word embedding  2分 [lab2]\n",
        "  创建word embedding model(表现word vectors eg: word2vec-CBOW, word2vec-Skip gram, fastText + Glove[lab2])\n",
        "  input embedding of your sequence model\n",
        "  [lab3,4]使用了one-hot vectors 作为inputs 给长序列模型\n",
        "  【要求做的】\n",
        "    1.数据的预处理（word embeddings[lab2;还要做解释说明]） NLTK Twitter dataset ( section 1 提供了) or/and 任何数据集（TED talk , Google News)\n",
        "    2.创建训练模型（for word embeddings) 创建训练模型；需要说清楚超参数【lab2（dimension of embeddings; window size, learning rate,etc.】 \n",
        "        note 任何word embeddings model[lab2] (word2vec-CBOW, word2vec-Skip gram, fasttext, glove)【证明】\n",
        "    3.训练模型： train model\n",
        "\n",
        "#Lexicon embedding 2分 （词典嵌入）\n",
        "  每个单词的积极或者消极的lexicon【从文件夹下载：opinion Lexicaon】(2006 positive and 4783 negative words)\n",
        "  每个单词要被改成： one-dimensional一维分类嵌入，三类。【eg:[not_exist(0), negative(1), positive(2);这个012类别 会在[2.3 的Bi-directional RNN 模型中用到]]\n",
        "  【如果你想用超过一维，或者不使用分类嵌入（categorical） 证明观点】\n",
        "\n",
        "# Bi-directional RNN Sequence Model 3分\n",
        "Many-to-One (N to 1) 建个n to 1 sequence model 为了探测感情（detect sentiment/emotion）你的模型应该是评价中选出的最佳模型（将在第3节评价【evaluation】中讨论）。\n",
        "  【做的】\n",
        "    1.apply/import word + lexicon embedding as input: 你要把训练好的词嵌入和词库嵌入合并起来，并应用到序列模型上    ##不太理解\n",
        "    2.1构建训练Sequence model: 构建Bi-directional RNN-based(Bi-RNN / Bi-LSTM / Bi-GRU) \n",
        "    2.2Many to One (N to 1) sequence model (N: word, One: sentiment- positive or negative)\n",
        "    3.描述超参【证明】【lab4,5】[the number of epochs, learning rate ……]\n",
        "    4.Train model  展示：training loss + number of epochs[lab4,lab5]\n",
        "\n",
        "    【不展示没分】\n",
        "'''\n",
        "'''\n",
        "【要做的】\n",
        "  1，word embedding module, 2，Lexicon Embedding module + 3，Bi-directional RNN sequence model\n",
        "\n",
        "  [从lab2,4,5随便选参数]embeddings_dimension ; learning_rate epochs;\n",
        "  [lab5 有模型构建]\n",
        "\n",
        "\n",
        "# word embedding  2分 [lab2]\n",
        "  创建word embedding model(表现word vectors eg: word2vec-CBOW, word2vec-Skip gram, fastText + Glove[lab2])\n",
        "  input embedding of your sequence model\n",
        "  [lab3,4]使用了one-hot vectors 作为inputs 给长序列模型\n",
        "  【要求做的】\n",
        "    1.数据的预处理（word embeddings[lab2;还要做解释说明]） NLTK Twitter dataset ( section 1 提供了) or/and 任何数据集（TED talk , Google News)\n",
        "    2.创建训练模型（for word embeddings) 创建训练模型；需要说清楚超参数【lab2（dimension of embeddings; window size, learning rate,etc.】 \n",
        "        note 任何word embeddings model[lab2] (word2vec-CBOW, word2vec-Skip gram, fasttext, glove)【证明】\n",
        "    3.训练模型： train model\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-"
      },
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *\n",
        "*要求您说明采用的是哪种模式（即Word2Vec与CBOW、FastText与SkipGram等），并说明您决定的理由*。【也就是三种都要做一做】"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings 字词嵌入的数据预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*\n",
        "您需要说明使用了哪些预处理技术，并说明您的决定的理由。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model. 建立单词嵌入模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*您需要说明超参数是如何决定的，并说明决定的理由。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfOocivpX7jz"
      },
      "source": [
        "#Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcAXL_Z3Y6OM",
        "outputId": "b3d4935d-c30b-4d46-883d-c4f0ab26144f"
      },
      "source": [
        "testing_data[0]#('neg', '@AndreaMarySmith very helpful .... Or will be once I stop crying :(')\n",
        "'''\n",
        "都是标签+句子\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', '@AndreaMarySmith very helpful .... Or will be once I stop crying :(')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfTnAoI4YtQz",
        "outputId": "09489ebb-97e1-4216-ed09-4736f25df7af"
      },
      "source": [
        "training_data[0]#('neg', 'hopeless for tmr :(')\n",
        "training_data[1]#('neg',\"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\")\n",
        "training_data[2]#('neg', '@Hegelbon That heart sliding into the waste basket. :(')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', '@Hegelbon That heart sliding into the waste basket. :(')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic3G1zAZYnen",
        "outputId": "3dd7c37c-14e3-497c-ff67-ef532489e6c0"
      },
      "source": [
        "len(testing_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTXrf6OYjGn"
      },
      "source": [
        "testing_data  #2000\n",
        "'''\n",
        " \"@NUFC_Vine he's gona miss Sheff Utd/York probably waiting for a visa which leaves gim with 1 friendly. Bet he starts season on the bench :(\"),\n",
        " ('neg', \"@ranaPTX_ BUT WHO WILL LEAD US WHEN YOU'RE GONE :((\"),\n",
        " ('neg', '@LottyStorer @charliee_catlin 😂😩😩 what a mess :('),\n",
        " ('neg', '@twinitisha irony is its more harmful .. :('),\n",
        " ('neg', '@DollyyDaydreams I miss you so much more :( xxx'),\n",
        " ('neg', 'no motivation :('),\n",
        " ('neg',\n",
        "  '@LiLGeekette but that is the point. Because it is YOLO I sort of die when lactose is in me... Ugh like 3 months pregnant bloating :('),\n",
        " (\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w17KNAbyYGW8",
        "outputId": "f412e2eb-4487-4c5f-e38e-037f73a958a9"
      },
      "source": [
        "len(training_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "b76arcTaX8st",
        "outputId": "dcecefb2-511f-42b6-860a-b3c297dd484b"
      },
      "source": [
        "training_data #8000\n",
        "'''\n",
        " ('neg', \"@rupexo @LordOfTheMics fuckkk it's a Wednesday :(\"),\n",
        " ('neg',\n",
        "  '@PetiteMistress DO IT! I want to start one for making small games, but I feel like I need to get a jump start before asking for support :('),\n",
        " ('neg', 'Justin where are you ? :( @justinbieber'),\n",
        " ('neg', \":( :( I'm gonna cry\"),\n",
        " ('neg',\n",
        "  \"@IanHallard @ArtsTheatreLDN I'm really excited but also really sad because once again I won't be able to see it :(\"),\n",
        " ('neg', \"I can't finish my Sanum today! :(\\n#LLAOLLAO #Dessert but too full\"),\n",
        " ('neg', '@jxstkatie @TheSvante i want foood :(('),\n",
        " ('neg', 'Last day :('),\n",
        " ('neg', '#UberIceCream was super! But we didnt get the glares :( @Uber_Pune'),\n",
        " ('neg', \"@ffyeahh i don't know how to do vines :(\"),\n",
        " ('neg',\n",
        "  \"@selenagomez you was Tweetin earlier this morning &amp; i wasn't here :( sad mood all the day :(\"),\n",
        " ('n\n",
        " '''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n (\\'neg\\', \"@rupexo @LordOfTheMics fuckkk it\\'s a Wednesday :(\"),\\n (\\'neg\\',\\n  \\'@PetiteMistress DO IT! I want to start one for making small games, but I feel like I need to get a jump start before asking for support :(\\'),\\n (\\'neg\\', \\'Justin where are you ? :( @justinbieber\\'),\\n (\\'neg\\', \":( :( I\\'m gonna cry\"),\\n (\\'neg\\',\\n  \"@IanHallard @ArtsTheatreLDN I\\'m really excited but also really sad because once again I won\\'t be able to see it :(\"),\\n (\\'neg\\', \"I can\\'t finish my Sanum today! :(\\n#LLAOLLAO #Dessert but too full\"),\\n (\\'neg\\', \\'@jxstkatie @TheSvante i want foood :((\\'),\\n (\\'neg\\', \\'Last day :(\\'),\\n (\\'neg\\', \\'#UberIceCream was super! But we didnt get the glares :( @Uber_Pune\\'),\\n (\\'neg\\', \"@ffyeahh i don\\'t know how to do vines :(\"),\\n (\\'neg\\',\\n  \"@selenagomez you was Tweetin earlier this morning &amp; i wasn\\'t here :( sad mood all the day :(\"),\\n (\\'n\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D7sHyZeYC0w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
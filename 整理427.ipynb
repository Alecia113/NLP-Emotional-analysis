{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "整理427.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Emotional-analysis/blob/main/%E6%95%B4%E7%90%86427.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJFkZQzwtE9q",
        "outputId": "bd90d178-4605-421b-94f2-0f4716b35ac4"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle #必要的\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\")) #必要的\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\")) #必要的\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data))) #8000\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data))) #2000\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n",
        "#LABEL: neg / SENTENCE: hopeless for tmr :("
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncjYnjYDE-Yn"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JEG1R2GsSXM",
        "outputId": "869d9dba-1eac-40e1-f894-23473116f4df"
      },
      "source": [
        "import torch\n",
        "from random import shuffle\n",
        "import pprint\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "train_data = [training_data[i][1] for i in range(len(training_data))]\n",
        "train_label = [training_data[i][0] for i in range(len(training_data))]\n",
        "test_data = [testing_data[i][1] for i in range(len(testing_data))]  \n",
        "test_label = [testing_data[i][0] for i in range(len(testing_data))] #neg pos\n",
        "\n",
        "\n",
        "zipped = zip(train_data,train_label)  \n",
        "Zipp = list(zipped) \n",
        "shuffle(Zipp)\n",
        "train_data = [context[0] for context in Zipp]\n",
        "train_label = [context[1] for context in Zipp]\n",
        "\n",
        "\n",
        "def preprocessing(data_text):\n",
        "  text = [s.lower() for s in data_text]\n",
        "\n",
        "\n",
        "  def remove_punctuation_re(x):\n",
        "      # Please complete this\n",
        "      x1 = re.sub(r'[^\\w\\s]','',x)\n",
        "      x2 = re.sub(r'\\d','',x1)\n",
        "      return x2\n",
        "\n",
        "  text_re = [remove_punctuation_re(s) for s in text]\n",
        "\n",
        "  tknzr = TweetTokenizer()\n",
        "  text_t=[]                                    #完全每句话的分词的集合。train_t[0][0]每个单词\n",
        "  for s in text_re:\n",
        "    text_re = tknzr.tokenize(s)              #每句话分别作了分词 text_train[0]第一句话的分词\n",
        "    text_t.append(text_re)  \n",
        "  '''表情的保留方式  因为字典中没表情，所以就不用表情了\n",
        "  def remove(x):\n",
        "    t = []\n",
        "    for i in range(len(x)):\n",
        "      t_sub = []        #是直接用空列表代替了\n",
        "      for j in range(len(x[i])):\n",
        "        if len(x[i][j])==0:\n",
        "          continue\n",
        "        if x[i][j] == \" \":\n",
        "          continue \n",
        "        else:\n",
        "          x[i][j] = re.sub(r'\\d','',x[i][j])                  #删除0-9\n",
        "          if len(x[i][j]) == 1 or len(x[i][j]) > 3:           #and + or 问题。\n",
        "            x[i][j] = re.sub(r'[^\\w\\s+]','', x[i][j])\n",
        "          if len(x[i][j]) == 0:\n",
        "            continue \n",
        "          else:\n",
        "            t_sub.append(x[i][j]) \n",
        "      t.append(t_sub) \n",
        "    return t\n",
        "\n",
        "  new_text = remove(text_t)\n",
        "  '''\n",
        "\n",
        "\n",
        "\n",
        "  stop_words = sw.words()\n",
        "  sww = sw.words()\n",
        "  text_stop=[]    #8000\n",
        "  for tokens in text_t:\n",
        "      filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "      text_stop.append(filtered_sentence)\n",
        "  return text_stop\n",
        "\n",
        "pre_train= preprocessing(train_data)  #对应后面，后面还得改一下 train_stem ;test_stem\n",
        "pre_test =preprocessing(test_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "sdsADvHD2wsc",
        "outputId": "f99cd26f-dec3-4038-dc01-56f644158973"
      },
      "source": [
        "'''\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize(past_text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  text_le = []\n",
        "  for tokens in past_text:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    text_le.append(lemma_sentence)\n",
        "  return text_le\n",
        "\n",
        "lem_train = lemmatize(pre_train)  #nobodies-->nobody\n",
        "lem_test = lemmatize(pre_test)\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nnltk.download('wordnet')\\nfrom nltk.stem import WordNetLemmatizer\\n\\ndef lemmatize(past_text):\\n  lemmatizer = WordNetLemmatizer()\\n\\n  text_le = []\\n  for tokens in past_text:\\n    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\\n    text_le.append(lemma_sentence)\\n  return text_le\\n\\nlem_train = lemmatize(pre_train)  #nobodies-->nobody\\nlem_test = lemmatize(pre_test)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBHbNhu7SVU"
      },
      "source": [
        "\n",
        "# stem\n",
        "from nltk.stem.porter import *\n",
        "def stemming(past_text):\n",
        "  stemmer = PorterStemmer()\n",
        "  train_stem = []\n",
        "  for i in range(len(past_text)):\n",
        "    singles = []\n",
        "    for plural in past_text[i]:\n",
        "      singles.append(stemmer.stem(plural))\n",
        "    train_stem.append(singles)\n",
        "  return train_stem\n",
        "\n",
        "stem_train = stemming(pre_train)  #nobodies -->nobodi\n",
        "stem_test = stemming(pre_test)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8qkfuG99-z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "6318e90d-c0d7-4ef6-8982-114434e71d55"
      },
      "source": [
        "#为什么要stem\n",
        "'''\n",
        "II = torch.load('save_stem.PATH')  # 'save_lem.PATH'\n",
        "O=model.load_state_dict(II['model_state_dict']) #成功\n",
        "optimizer.load_state_dict(II['optimizer_state_dict'])\n",
        "PP = II['epoch'] #19\n",
        "LL = II['loss'] #0.6927\n",
        "P2 = II['predicted2']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "O2 = II['outputs2']   \n",
        "\n",
        "tensor([[ 0.1570, -0.1038],\n",
        "        [-0.2550,  0.3335],\n",
        "        [ 0.1559, -0.0992],\n",
        "        ...,\n",
        "        [ 0.1649, -0.1156],\n",
        "        [-0.1996,  0.2628],\n",
        "        [ 0.1186, -0.0533]], requires_grad=True)\n",
        "\n",
        "P = II['predicted'] #tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,// 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
        "ACC = II['acc'] #0.61  看起来是存了期中的一个\n",
        "TL = II['train_loss'] #6.557632803916931  看起来是存了期中的一个\n",
        "#不再存直接跑是上次的状态\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "print(O2)\n",
        "print(P)\n",
        "print(ACC)\n",
        "print(TL)\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nII = torch.load('save_stem.PATH')  # 'save_lem.PATH'\\nO=model.load_state_dict(II['model_state_dict']) #成功\\noptimizer.load_state_dict(II['optimizer_state_dict'])\\nPP = II['epoch'] #19\\nLL = II['loss'] #0.6927\\nP2 = II['predicted2']   #tensor([0, 1, 0,  ..., 0, 1, 0])\\nO2 = II['outputs2']   \\n\\ntensor([[ 0.1570, -0.1038],\\n        [-0.2550,  0.3335],\\n        [ 0.1559, -0.0992],\\n        ...,\\n        [ 0.1649, -0.1156],\\n        [-0.1996,  0.2628],\\n        [ 0.1186, -0.0533]], requires_grad=True)\\n\\nP = II['predicted'] #tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,// 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\\nACC = II['acc'] #0.61  看起来是存了期中的一个\\nTL = II['train_loss'] #6.557632803916931  看起来是存了期中的一个\\n#不再存直接跑是上次的状态\\nprint(PP)\\nprint(LL)\\nprint(P2)\\nprint(O2)\\nprint(P)\\nprint(ACC)\\nprint(TL)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdAYqeB78nnP"
      },
      "source": [
        "想测试哪个预处理好用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tz5FX-8nuf"
      },
      "source": [
        "sentences = stem_train + stem_test      #\n",
        "'''\n",
        "from gensim.models import Word2Vec\n",
        "wv_cbow_model = Word2Vec(sentences= sentences, size=100, window=5, min_count=2, workers=2, sg=0)\n",
        "wv_cbow_model.save(\"cbow.model\")\n",
        "#cbow = Word2Vec.load(\"./cbow.model\")  \n",
        "'''\n",
        "\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=100, window=5, min_count=2, workers=2, sg=1)\n",
        "ft_sg_model.save('ft_sg.model')\n",
        "ft_sg = FastText.load(\"./ft_sg.model\")  \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26DkKO_M3ut"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhjNn6-G9QQN",
        "outputId": "b5b74048-a441-4828-ee7f-2feec0c49c84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "not_words = [w.strip() for w in open('/content/drive/MyDrive/NLP/A1/E/opinion-lexicon-English/negative-words.txt', 'r', encoding=\"ISO-8859-1\").readlines()]\n",
        "#处理文件 -\n",
        "j = -1\n",
        "for i in not_words:\n",
        "  j +=1\n",
        "  if i == '2-faced':\n",
        "    break\n",
        "neg = not_words[j:]\n",
        "\n",
        "#处理文件+\n",
        "sure_words = [w.strip() for w in open('/content/drive/MyDrive/NLP/A1/E/opinion-lexicon-English/positive-words.txt', 'r', encoding=\"ISO-8859-1\").readlines()]\n",
        "\n",
        "j = -1\n",
        "for i in sure_words:\n",
        "  j +=1\n",
        "  if i == 'a+':\n",
        "    break\n",
        "\n",
        "pos = sure_words[j:]\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "'''\n",
        "def lem(past_text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  text_le = []\n",
        "\n",
        "  lemma_sentence = [lemmatizer.lemmatize(w) for w in past_text ]\n",
        "  text_le.append(lemma_sentence)\n",
        "  return text_le\n",
        "neg_new = lem(neg)\n",
        "pos_new = lem(pos)\n",
        "'''\n",
        "\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "def stem(past_text):\n",
        "  stemmer = PorterStemmer()\n",
        "  text_le = []\n",
        "  stemmer_sentence = [stemmer.stem(plural) for plural in past_text ]\n",
        "  text_le.append(stemmer_sentence)\n",
        "  return text_le\n",
        "\n",
        "neg_new = stem(neg)\n",
        "pos_new = stem(pos)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5QvuNj0-rjc"
      },
      "source": [
        "#只是原来的，因为neg就是单词了。不能再neg[0]\n",
        "'''\n",
        "大改动\n",
        "'''\n",
        "def match(text):\n",
        "  sum_match= [] #train_match\n",
        "  for i in range(len(text)):  #train_stem  取出一句话\n",
        "    match = []\n",
        "    for j in range(len(text[i])):   #取出一句话中的单词\n",
        "      if text[i][j] in neg_new[0]:    #neg_stem  第几句话的第几个单词\n",
        "        match.append(1)   #neg 1\n",
        "      elif text[i][j] in pos_new[0]:  #pos_stem\n",
        "        match.append(2) #pos 2\n",
        "      else:\n",
        "        match.append(0)\n",
        "    sum_match.append(match)\n",
        "  return sum_match\n",
        "train_match = match(stem_train)  #这在不断的改变 train_stem  原本的话都是000\n",
        "test_match = match(stem_test)\n",
        "#只有stem好用\n",
        "#print(train_match)\n",
        "#print(test_match[:5])\n",
        "#就是把我原本的要训练的话，纷纷用012 表示出来。每个词告诉他是积极消极还是不在"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlz1-F34DrLZ",
        "outputId": "4fd253e6-72ce-4313-f0c5-6e63658d4ffd"
      },
      "source": [
        "#实现的是concat的功能。\n",
        "'''\n",
        "大改动\n",
        "'''\n",
        "def input(text,text_match):\n",
        "  DF = {}\n",
        "\n",
        "  for tokensized_doc in text: \n",
        "      for term in np.unique(tokensized_doc):\n",
        "          try:\n",
        "              DF[term] +=1\n",
        "          except:\n",
        "              DF[term] =1\n",
        "  input = []\n",
        "  for w in range(len(text)):                #8000 0-7999\n",
        "    num = 0\n",
        "    new = []\n",
        "    for t in text[w]:                       #一句话 #t就是这个单词  #13\n",
        "      match = []                            #目前这个就是第一句话的match\n",
        "      if DF[t] >= 2:                         #0-12     这个min_count\n",
        "        word_vec = ft_sg[t].tolist()\n",
        "        match.append(text_match[w][num])     #w 012 []   #IndexError: list index out of range 后面的num问题\n",
        "        new_embedding = word_vec + match       #sent_embedding\n",
        "        new.append(new_embedding)\n",
        "      num += 1\n",
        "      if num >= len(text_match[w]):\n",
        "        break\n",
        "    input.append(new)                          # input == train_embedding\n",
        "                                                #print(input[0][0]) #input 8000一句话 3一个词 101 vec+0、1\n",
        "\n",
        "  return input\n",
        "\n",
        "input_train = input(stem_train,train_match)         #train_stem变化\n",
        "input_test = input(stem_test,test_match)           #test_stem变化\n",
        "#pprint.pprint(input_train[:1])\n",
        "#print('\\n')\n",
        "#pprint.pprint(input_test[:1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXtkQuOXDrWy"
      },
      "source": [
        "#主要是为了得到n_class\n",
        "unique_labels = np.unique(train_label)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "label_train_encoded = lEnc.fit_transform(train_label)         #lEnc.transform(unique_labels)[0]+1\n",
        "label_test_encoded = lEnc.fit_transform(test_label)\n",
        "n_class = len(unique_labels)                                   #主要是为了得到n_class  #n_class = np.unique(unique_labels).shape[0] #2相当于\n",
        "\n",
        "#要确定最大长度是多少 [lab4]\n",
        "doc_length_list = []                                              #会得到8000句话\n",
        "sum_text = pre_train + pre_test     #train_stem + test_stem要改的\n",
        "maxlength = 0\n",
        "for doc in sum_text:\n",
        "    doc_length_list.append(len(doc))                                  #每句话多少个分词\n",
        "\n",
        "for index in range(len(doc_length_list)):\n",
        "  if doc_length_list[index] > maxlength:\n",
        "    maxlength = doc_length_list[index]\n",
        "    max_index = index\n",
        "#print(doc_length_list[max_index]) #21\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXHPhY33Drea"
      },
      "source": [
        "#超参\n",
        "\n",
        "seq_length = doc_length_list[max_index]           #（22）应该是我规定的最大序列长度512那种  maxlength 增加减少那种 #一句话最长多少个词\n",
        "n_input = len(input_train[0][0])              #101 无所谓的反正都是101维度\n",
        "n_class = len(unique_labels)                  #2\n",
        "\n",
        "n_hidden = 200                                  #128\n",
        "batch_size = 800                                #500 \n",
        "total_epoch = 20                                #10\n",
        "learning_rate = 0.001                           # 1e-4 == 0.001 le-3 == 0.01 #0.1  0.05\n",
        "\n",
        "\n",
        "#把输入的规格都定成一样的， 变成句子长度是22，单词维度是101\n",
        "\n",
        "def sent_encoded(input_text):\n",
        "  sent_encoded = []\n",
        "  for m in range(len(input_text)):                       # m 1-8000 #训练集  input_train  input_test\n",
        "    \n",
        "    encoded = [] \n",
        "    zero = []\n",
        "    if len(input_text[m]) < seq_length:\n",
        "      zero = [len(input_text[0][0])*[0]] * (seq_length - len(input_text[m]))#补充到22 最长 # 不单单加0； 还是需要变成101维度\n",
        "      encoded = input_text[m] + zero\n",
        "    else:\n",
        "      encoded = input_text[m]\n",
        "    sent_encoded.append(encoded)                                              #要重新添加。 句子已经全变成22了 \n",
        "\n",
        "  sent_encoded = np.array(sent_encoded)\n",
        "  return sent_encoded\n",
        "\n",
        "sent_encoded_train = sent_encoded(input_train)\n",
        "sent_encoded_test = sent_encoded(input_test)\n",
        "#print(sent_encoded_train[:1]) #(8000, 22, 101)\n",
        "#print(sent_encoded_test[:1])#(2000, 22, 101)\n",
        "\n",
        "\n",
        "#还是Bi-LSTM准确度更高，因为LSTM处理了记忆丢失的问题。\n",
        "'''\n",
        "要写个LSTM比RNN好在哪里。\n",
        "'''\n",
        "# 模型Bi-RNN\n",
        "\n",
        "\n",
        "'''\n",
        "class Bi_RNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_RNN_Model, self).__init__()\n",
        "        # set the bidirectional to True\n",
        "        self.rnn = nn.RNN(n_input, n_hidden, batch_first =True, bidirectional=True) #, dropout = 0.5\n",
        "        self.linear = nn.Linear(2*n_hidden,n_class) #n_class = 3 \n",
        "        #https://stackoverflow.com/questions/60259836/cnn-indexerror-target-2-is-out-of-bounds 虽然是01分类但是，pytorch 要进行012 \n",
        "    def forward(self, x):        \n",
        "        x, h_n = self.rnn(x)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        output = self.linear(hidden_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "#LSTM-BI\n",
        "class Bi_LSTM_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_LSTM_Model, self).__init__()\n",
        "        #self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        # Initialize the Embedding layer with the lookup table we created \n",
        "        #self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
        "        # Optional: set requires_grad = False to make this lookup table untrainable\n",
        "        #self.emb.weight.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True, bidirectional=True)\n",
        "        self.linear = nn.Linear(n_hidden*2, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the embeded tensor\n",
        "        #x = self.emb(x)        \n",
        "        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
        "        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "        lstm_out, (h_n,c_n) = self.lstm(x)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        return z\n",
        "\n",
        "\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "'''\n",
        "#Bi-LSTM\n",
        "# Move the model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_LSTM_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hehwx11DrlL",
        "outputId": "7a4421ba-1160-487b-bc51-bac14005503b"
      },
      "source": [
        "# Please comment your code\n",
        "#[lab4][lab5]\n",
        "#检查输出节点数与标签数是否一致。\n",
        "#并且将输出节点数num_outputs更改为标签数。\n",
        "# 因为我之前设置的class 类别是 0，1；所以这里也需要改成0，1 要不然后面会出现 target超出范围的错误。\n",
        "def label(label):   \n",
        "  label = np.array(label) #train_label\n",
        "  #neg 1 pos2\n",
        "  lab = []  #label_train\n",
        "  for tag in label:\n",
        "    if tag == 'neg':\n",
        "      lab.append(0)\n",
        "    else:\n",
        "      lab.append(1)\n",
        "  #print(label_train)\n",
        "  #print(len(label_train))\n",
        "  lab = np.array(lab)\n",
        "  #test_label\n",
        "  return lab\n",
        "\n",
        "label_train = label(train_label)\n",
        "label_test = label(test_label)\n",
        "\n",
        "#要开始训练。 optimizer +loss+ backward\n",
        "\n",
        "#处理\n",
        "#sent_encoded_train = sent_encoded(input_train)\n",
        "#sent_encoded_test = sent_encoded(input_test)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,sent_encoded_train.shape[0],batch_size):\n",
        "        input_batch = sent_encoded_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])] #256,22,101\n",
        "        target_batch = label_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])]  #256\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)  #256,22,101\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device) #256\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch)   #13,512,25 #256，22，101    #256 2 #500,22,101\n",
        "        loss = criterion(outputs, target_batch_torch) #256 2    256 out,y\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, 1)  # 输出值为元祖获取第二个tensor        \n",
        "    acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "    print('Epoch: %d, train loss: %.5f, train_acc:%.2f'%(epoch + 1, train_loss, acc))\n",
        "\n",
        "print('Finished Training')\n",
        "#这块是在后面评估，可删\n",
        "## Prediction\n",
        "\n",
        "model.eval()\n",
        "outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "predicted2 = torch.argmax(outputs2, 1)\n",
        "\n",
        "\n",
        "print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "#是改变了batch\n",
        "#3.59s"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, train loss: 6.87348, train_acc:0.60\n",
            "Epoch: 2, train loss: 6.75540, train_acc:0.61\n",
            "Epoch: 3, train loss: 6.66681, train_acc:0.59\n",
            "Epoch: 4, train loss: 6.60746, train_acc:0.60\n",
            "Epoch: 5, train loss: 6.54393, train_acc:0.62\n",
            "Epoch: 6, train loss: 6.47251, train_acc:0.61\n",
            "Epoch: 7, train loss: 6.39154, train_acc:0.62\n",
            "Epoch: 8, train loss: 6.34640, train_acc:0.63\n",
            "Epoch: 9, train loss: 6.30027, train_acc:0.62\n",
            "Epoch: 10, train loss: 6.28284, train_acc:0.64\n",
            "Epoch: 11, train loss: 6.25530, train_acc:0.63\n",
            "Epoch: 12, train loss: 6.24696, train_acc:0.64\n",
            "Epoch: 13, train loss: 6.22470, train_acc:0.63\n",
            "Epoch: 14, train loss: 6.22778, train_acc:0.65\n",
            "Epoch: 15, train loss: 6.20968, train_acc:0.63\n",
            "Epoch: 16, train loss: 6.19385, train_acc:0.65\n",
            "Epoch: 17, train loss: 6.17131, train_acc:0.65\n",
            "Epoch: 18, train loss: 6.16891, train_acc:0.65\n",
            "Epoch: 19, train loss: 6.13971, train_acc:0.66\n",
            "Epoch: 20, train loss: 6.13612, train_acc:0.64\n",
            "Finished Training\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5973    0.6600    0.6271      1000\n",
            "           1     0.6201    0.5550    0.5858      1000\n",
            "\n",
            "    accuracy                         0.6075      2000\n",
            "   macro avg     0.6087    0.6075    0.6064      2000\n",
            "weighted avg     0.6087    0.6075    0.6064      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcGQrTXxDrrT"
      },
      "source": [
        "'''\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'predicted2':predicted2,\n",
        "            'outputs2':outputs2,\n",
        "            'predicted':predicted,\n",
        "            'acc':acc,\n",
        "            'train_loss':train_loss,\n",
        "            \n",
        "            }, 'save_stem.PATH')\n",
        "\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_AI4DPgdVWb",
        "outputId": "89cb2a60-ada6-4940-a28d-f417c1bd7c4d"
      },
      "source": [
        "'''\n",
        "II = torch.load('save_stem.PATH')\n",
        "O=model.load_state_dict(II['model_state_dict']) #成功\n",
        "optimizer.load_state_dict(II['optimizer_state_dict'])\n",
        "PP = II['epoch'] #19\n",
        "LL = II['loss'] #0.6927\n",
        "P2 = II['predicted2']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "O2 = II['outputs2']   \n",
        "'''\n",
        "'''\n",
        "tensor([[ 0.1570, -0.1038],\n",
        "        [-0.2550,  0.3335],\n",
        "        [ 0.1559, -0.0992],\n",
        "        ...,\n",
        "        [ 0.1649, -0.1156],\n",
        "        [-0.1996,  0.2628],\n",
        "        [ 0.1186, -0.0533]], requires_grad=True)\n",
        "        '''\n",
        "'''\n",
        "P = II['predicted'] #tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,// 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
        "ACC = II['acc'] #0.61  看起来是存了期中的一个\n",
        "TL = II['train_loss'] #6.557632803916931  看起来是存了期中的一个\n",
        "#不再存直接跑是上次的状态\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "print(O2)\n",
        "print(P)\n",
        "print(ACC)\n",
        "print(TL)\n",
        "'''"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n",
            "tensor(0.6148, requires_grad=True)\n",
            "tensor([1, 0, 0,  ..., 0, 1, 0])\n",
            "tensor([[-0.3060,  0.2420],\n",
            "        [ 0.0853, -0.1035],\n",
            "        [ 0.1225, -0.1451],\n",
            "        ...,\n",
            "        [ 0.1069, -0.1321],\n",
            "        [-0.1672,  0.0690],\n",
            "        [ 0.3293, -0.3770]], requires_grad=True)\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 1, 0, 1])\n",
            "0.64125\n",
            "6.1361247301101685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xCguRBsHeh"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 3 - Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLBzHObsvvM"
      },
      "source": [
        "## 3.1. Word Embedding Evaluation\n",
        "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJFNdISFsjIV",
        "outputId": "41a4711f-6f5b-4005-8c22-b2cf6d17a844"
      },
      "source": [
        "#要用语义，句法词关系来测试。[lab5-word_analogy_evaluation]\n",
        "#https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=EVk7tjwvhl-6\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from lxml import etree \n",
        "import pprint\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n",
        "\n",
        "\n",
        "#data preprocessing\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='ISO-8859-1')\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "#print(sentences[:10])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qo9ebqQslTP"
      },
      "source": [
        "#gensim word2vec #W2V-cbow\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "wv_cbow_model.wv.save_word2vec_format('ted_cbow_w2v.txt', binary=False)\n",
        "vectors_wv_cbow=\"/content/ted_cbow_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#W2v-skip gram\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "wv_sg_model.wv.save_word2vec_format('ted_sg_w2v.txt', binary=False)\n",
        "vectors_wv_sg=\"/content/ted_sg_w2v.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "#FastText- sg\n",
        "from gensim.models import FastText\n",
        "ft_sg_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "ft_sg_model.wv.save_word2vec_format('ted_sg_ft.txt', binary=False)\n",
        "vectors_ft_sg=\"/content/ted_sg_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n",
        "# fasttext - cbow\n",
        "ft_cbow_model = FastText(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "ft_cbow_model.wv.save_word2vec_format('ted_cbow_ft.txt', binary=False)\n",
        "vectors_ft_cbow=\"/content/ted_cbow_ft.txt\"    #打开训练好的文件。vectors_file\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjHhbjSUsnBq",
        "outputId": "ac743f12-8d44-4a31-fc24-fd82a6f1b700"
      },
      "source": [
        "#匹配\n",
        "def open_files(file_name):\n",
        "  with open(file_name, 'r') as f:    #vectors_file == vectors_wv_cbow  ==file_name\n",
        "    vectors = {}\n",
        "    for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "      vals = line.rstrip().split(' ')\n",
        "      vectors[vals[0]] = [x for x in vals[1:]]\n",
        "\n",
        "\n",
        "  vocab_words=list(vectors.keys())\n",
        "  vocab_size = len(vocab_words)\n",
        "  print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "  # create word->index and index->word converter  \n",
        "  vocab = {w: idx for idx, w in enumerate(vocab_words)} #每个单词对应个index\n",
        "  ivocab = {idx: w for idx, w in enumerate(vocab_words)}  #每个index对应一个单词\n",
        "\n",
        "\n",
        "  # create the embedding matrix of shape (vocab_size, dim)\n",
        "  vector_dim = len(vectors[ivocab[0]])  #100\n",
        "  W = np.zeros((vocab_size, vector_dim))    #4325.100\n",
        "  for word, v in vectors.items():\n",
        "      if word == '<unk>' or word == '':   #我加的\n",
        "          continue\n",
        "      \n",
        "      W[vocab[word], :] = v   #100 102\n",
        "\n",
        "  # normalize each word vector to unit length\n",
        "  # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "  W_norm = np.zeros(W.shape)\n",
        "  d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "  W_norm = (W.T / d).T\n",
        "  \n",
        "  return W, vocab, W_norm\n",
        "#W, vocab\n",
        "\n",
        "#W,vocab = open_files(vectors_wv_cbow) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "#W,vocab,W_norm = open_files(vectors_wv_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n",
        "W,vocab,W_norm = open_files(vectors_ft_sg) #vectors_wv_sg  #vectors_ft_sg #vectors_ft_cbow\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  21613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvkoHvZnsqTm",
        "outputId": "ffc943ec-c127-4803-d3f4-6e5a962b09b0"
      },
      "source": [
        "\n",
        "#开始句法文本分析各种的功能语句。\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "def evaluate_vectors(W, vocab, prefix='./content/GloVe/eval/question-data/'):#/content/GloVe/eval/question-data\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions  计算正确的语义问题\n",
        "    correct_syn = 0; # count correct syntactic questions 计算正确的句法问题\n",
        "    correct_tot = 0 # count correct questions   计算正确的问题\n",
        "    count_sem = 0; # count all semantic questions 计算所有语义问题\n",
        "    count_syn = 0; # count all syntactic questions  计算所有语法问题\n",
        "    count_tot = 0 # count all questions   计算所有问题  \n",
        "    full_count = 0 # count all questions, including those with unknown words    计算所有问题包括不知道n个单词的问题\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1) #216\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "\n",
        "#可视化展示\n",
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')#/content/GloVe/eval/question-data\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %    #看到的问题\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %  #语义准确度 绿色的\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' % #语法准确度 蓝色的\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
        "  #总准确度红线"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GloVe'...\n",
            "remote: Enumerating objects: 595, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 595 (delta 0), reused 1 (delta 0), pack-reused 592\u001b[K\n",
            "Receiving objects: 100% (595/595), 222.33 KiB | 10.11 MiB/s, done.\n",
            "Resolving deltas: 100% (338/338), done.\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.37% (1/272)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 1.97% (10/507)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/28)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 1.47% (8/545)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 33.33% (114/342)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 77.46% (629/812)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 83.99% (425/506)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 78.45% (1045/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 85.98% (650/756)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 83.06% (824/992)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 40.99% (348/849)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 38.33% (568/1482)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 67.85% (631/930)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 74.34% (562/756)\n",
            "Questions seen/total: 51.72% (10109/19544)\n",
            "Semantic accuracy: 7.85%  (133/1694)\n",
            "Syntactic accuracy: 67.52%  (5682/8415)\n",
            "Total accuracy: 57.52%  (5815/10109)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u20BOjKGst_n"
      },
      "source": [
        "'''\n",
        "import torch\n",
        "torch.save({\n",
        "    'correct_sem': correct_sem, \n",
        "    'correct_syn': correct_syn,\n",
        "    'correct_tot': correct_tot,\n",
        "    'count_sem': count_sem,\n",
        "    'count_syn': count_syn,\n",
        "    'count_tot': count_tot,\n",
        "    'full_count': full_count   \n",
        "    }, 'ft_sg.PATH')   #'w2v_cbow.PATH' ; 'w2v_sg.PATH' ;'ft_sg.PATH' ; 'ft_cbow.PATH'\n",
        "    '''"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXMQ-ZLQtJOl",
        "outputId": "0ab4e960-8f53-4d9d-a87b-287715dfa2e2"
      },
      "source": [
        "'''\n",
        "import torch\n",
        "data_ft_sg = torch.load('ft_sg.PATH')\n",
        "PP = data_ft_sg['correct_sem'] #19\n",
        "LL = data_ft_sg['correct_syn'] #0.6927\n",
        "P2 = data_ft_sg['count_sem']   #tensor([0, 1, 0,  ..., 0, 1, 0])\n",
        "\n",
        "print(PP)\n",
        "print(LL)\n",
        "print(P2)\n",
        "'''\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "133\n",
            "5682\n",
            "1694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 3.2. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYjd6VWKsPiq",
        "outputId": "b093d7bf-9c05-414f-c3be-8d4b98f45695"
      },
      "source": [
        "# Please comment your code\n",
        "#[lab4]在展示report说明性能。\n",
        "\n",
        "## Prediction\n",
        "\n",
        "model.eval()\n",
        "outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "predicted2 = torch.argmax(outputs2, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "# More details can be found from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "#是改变了batch\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5973    0.6600    0.6271      1000\n",
            "           1     0.6201    0.5550    0.5858      1000\n",
            "\n",
            "    accuracy                         0.6075      2000\n",
            "   macro avg     0.6087    0.6075    0.6064      2000\n",
            "weighted avg     0.6087    0.6075    0.6064      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByStUsX8yR-f"
      },
      "source": [
        "def train_model(criterion, model, optimizer):\n",
        "  for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,sent_encoded_train.shape[0],batch_size):\n",
        "        input_batch = sent_encoded_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])] #256,22,101\n",
        "        target_batch = label_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])]  #256\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)  #256,22,101\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device) #256\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch)   #13,512,25 #256，22，101    #256 2 #500,22,101\n",
        "        loss = criterion(outputs, target_batch_torch) #256 2    256 out,y\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, 1)  # 输出值为元祖获取第二个tensor        \n",
        "    acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "    #print('Epoch: %d, train loss: %.5f, train_acc:%.2f'%(epoch + 1, train_loss, acc))\n",
        "\n",
        "  #print('Finished Training')\n",
        "  #这块是在后面评估，可删\n",
        "  ## Prediction\n",
        "\n",
        "  model.eval()\n",
        "  outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "  predicted2 = torch.argmax(outputs2, 1)\n",
        "  return predicted2\n",
        "#=======\n",
        "  from sklearn.metrics import classification_report\n",
        "  print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "  #是改变了batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWHL9eBayCLs"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "#Bi-RNN\n",
        "\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "model1 = Bi_RNN_Model().to(device)\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "\n",
        "pre_RNN = train_model(criterion1,model1,optimizer1)\n",
        "print(classification_report(label_test, pre_RNN.cpu().numpy(),digits=4))\n",
        "\n",
        "#Bi-LSTM\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "model2 = Bi_LSTM_Model().to(device)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "\n",
        "pre_LSTM = train_model(criterion2,model2,optimizer2)\n",
        "print(classification_report(label_test, pre_LSTM.cpu().numpy(),digits=4))\n",
        "#是改变了batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRdVntwY15It"
      },
      "source": [
        "3.2end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn0VqXdWsTCr"
      },
      "source": [
        "#备用\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "#Bi-LSTM\n",
        "# Move the model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_LSTM_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,sent_encoded_train.shape[0],batch_size):\n",
        "        input_batch = sent_encoded_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])] #256,22,101\n",
        "        target_batch = label_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])]  #256\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)  #256,22,101\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device) #256\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch)   #13,512,25 #256，22，101    #256 2 #500,22,101\n",
        "        loss = criterion(outputs, target_batch_torch) #256 2    256 out,y\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, 1)  # 输出值为元祖获取第二个tensor        \n",
        "    acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "    print('Epoch: %d, train loss: %.5f, train_acc:%.2f'%(epoch + 1, train_loss, acc))\n",
        "\n",
        "print('Finished Training')\n",
        "#这块是在后面评估，可删\n",
        "## Prediction\n",
        "\n",
        "model.eval()\n",
        "outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "predicted2 = torch.argmax(outputs2, 1)\n",
        "\n",
        "\n",
        "print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "#是改变了batch"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 3.3. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.* Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tH1Zt0zssWVw",
        "outputId": "0a6513f8-73e3-473d-e361-370f06d76a57"
      },
      "source": [
        "# Please comment your code\n",
        "#先把f1取出来，然后每训练一次存一次。\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(label_test, predicted2.cpu().numpy(),average='macro') #f1出来了。 真实，预测，没跑一次就一个结果。\n",
        "'''\n",
        "torch.save(model,'save.pt')\n",
        "model.load_state_dict(torch.load(\"save.pt\"))  #model.load_state_dict()函数把加载的权重复制到模型的权重中去\n",
        "'''\n",
        "\n",
        "#能一次存很多的东西。\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'save.PATH')\n",
        "\n",
        "II = torch.load('save.PATH')\n",
        "O=model.load_state_dict(II['model_state_dict']) \n",
        "optimizer.load_state_dict(II['optimizer_state_dict'])\n",
        "PP = II['epoch'] #19\n",
        "LL = II['loss'] #0.6927\n",
        "\n",
        "model.eval()\n",
        "model.train()\n",
        "\n",
        "'''\n",
        "在保存用于推理或者继续训练的常规检查点的时候，除了模型的state_dict之外，还必须保存其他参数。保存优化器的state_dict也非常重要，因为它包含了模型在训练时候优化器的缓存和参数。除此之外，还可以保存停止训练时epoch数，最新的模型损失，额外的torch.nn.Embedding层等。\n",
        "\n",
        "要保存多个组件，则将它们放到一个字典中，然后使用torch.save()序列化这个字典。一般来说，使用.tar文件格式来保存这些检查点。\n",
        "\n",
        "加载各个组件，首先初始化模型和优化器，然后使用torch.load()加载保存的字典，然后可以直接查询字典中的值来获取保存的组件。\n",
        "\n",
        "同样，评估模型的时候一定不要忘了调用model.eval()。\n",
        "'''\n",
        "\n",
        "\n",
        "#  x是epoch y是f1 测试集\n",
        "import matplotlib.pyplot as plt\n",
        "#for epoch in range(total_epoch):  \n",
        "#x = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "x = [epoch for epoch in range(8)]\n",
        "y = [17, 24, 29, 36, 38, 47, 59, 80]\n",
        "plt.plot(x,y,'rs-',markersize =2) # 那个点\n",
        "plt.show()\n",
        "# precision: TP/(TP+FP)\n",
        "#recall\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ6sH-5Tenyb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSDpkV-Ven70"
      },
      "source": [
        "超参数测试（2分）。\n",
        "你要提供线图，显示超参数测试（用测试数据集），\n",
        "并解释基于你选择的学习率的最佳历时数。    \n",
        "你可以有多个不同学习率的图。在图中，x轴将是epoch的数量，y轴将是f1。\n",
        "注意，如果你不在ipynb文件中显示它，它将不会被标记。\n",
        "\n",
        "'''\n",
        "何时学习率最好。 x轴将是epoch的数量，y轴将是f1。\n",
        "1搞个学习率的list[1,2,3,4,5]\n",
        "\n",
        "学习率不同线list =[]\n",
        "for 学习率\n",
        "  list每次epoch的f1 = []\n",
        "  for epoch\n",
        "    lstm 模型 求f1 ; 把它加进list每次epoch的f1（append）  #这个是每次epoch对应的f1点集。==这俩是一一对应的 （还差个epoch的list）\n",
        "  append 学习率不同线list =[] #这个用来画不同的线\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0k8JzVwfzcQ"
      },
      "source": [
        "3。3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL_h1X5zf0k5"
      },
      "source": [
        "seq_length = doc_length_list[max_index]           #（22）应该是我规定的最大序列长度512那种  maxlength 增加减少那种 #一句话最长多少个词\n",
        "n_input = len(input_train[0][0])              #101 无所谓的反正都是101维度\n",
        "n_class = len(unique_labels)                  #2\n",
        "\n",
        "n_hidden = 512                                  #128 64 ；128； 256   512\n",
        "batch_size = 512                                #500 \n",
        "total_epoch = 15                                #10   1000句话 20 ~500\n",
        "learning_rate = [1,0.5,0.1,0.05,0.01,0.005,0.001]                          # 1e-4 == 0.001 le-3 == 0.01 #0.1  0.05\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0nPGh5Ki2qw",
        "outputId": "e6811875-c875-4e3c-a0f4-250cfa76eb49"
      },
      "source": [
        "lr"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQENab9Sgn2r"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "line_lr = []\n",
        "for learn in range(len(learning_rate)):\n",
        "  epoch_f1 = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model = Bi_LSTM_Model().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate[learn])\n",
        "\n",
        "  for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,sent_encoded_train.shape[0],batch_size):\n",
        "        input_batch = sent_encoded_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])] #256,22,101\n",
        "        target_batch = label_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])]  #256\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)  #256,22,101\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device) #256\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch)   #13,512,25 #256，22，101    #256 2 #500,22,101\n",
        "        loss = criterion(outputs, target_batch_torch) #256 2    256 out,y\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, 1)  # 输出值为元祖获取第二个tensor        \n",
        "    acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "  \n",
        "    model.eval()\n",
        "    outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "    predicted2 = torch.argmax(outputs2, 1)\n",
        "    #print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "    f1 = f1_score(label_test, predicted2.cpu().numpy(),average='macro') #f1出来了。 真实，预测，没跑一次就一个结果。\n",
        "    epoch_f1.append(f1)\n",
        "    #print(epoch_f1)\n",
        "  line_lr.append(epoch_f1)\n",
        "  print(line_lr)\n",
        "\n",
        "#一次一分钟。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1ALvu4Ew3ZM"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2JT40elOjI1b",
        "outputId": "20c6a1dc-d643-4bdb-f017-19dac49d5a87"
      },
      "source": [
        "#  x是epoch y是f1 测试集  command+/\n",
        "import matplotlib.pyplot as plt\n",
        "#for epoch in range(total_epoch):  \n",
        "#x = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "#y = [17, 24, 29, 36, 38, 47, 59, 80]\n",
        "#[1,0.5,0.1,0.05,0.01,0.005,0.001] \n",
        "x = [epoch for epoch in range(total_epoch)]\n",
        "# y1 = [f1 for f1 in line_lr[0]]\n",
        "# y2 = [f1 for f1 in line_lr[1]]\n",
        "# y3 = [f1 for f1 in line_lr[2]]\n",
        "# y4 = [f1 for f1 in line_lr[3]]\n",
        "# y5 = [f1 for f1 in line_lr[4]]\n",
        "# y6 = [f1 for f1 in line_lr[5]]\n",
        "# y7 = [f1 for f1 in line_lr[6]]\n",
        "plt.plot(x,y1,'rs-',markersize =3,label = 'lr=1') # 那个点  红色尺寸点\n",
        "\n",
        "# plt.plot(x,y2,'g-',markersize =3, label = 'lr = 0.5') # 那个点  红色尺寸点\n",
        "# plt.plot(x,y3,'b-',markersize =3, label = 'lr = 0.1') # 那个点  红色尺寸点\n",
        "# plt.plot(x,y4,'y-',markersize =3, label = 'lr = 0.05') # 那个点  红色尺寸点\n",
        "# plt.plot(x,y5,'c-',markersize =3, label = 'lr = 0.01') # 那个点  红色尺寸点\n",
        "# plt.plot(x,y6,'m-',markersize =3, label = 'lr = 0.005') # 那个点  红色尺寸点\n",
        "# plt.plot(x,y7,'k-',markersize =3, label = 'lr = 0.001') # 那个点  红色尺寸点\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('f1')\n",
        "plt.title('F1 cases at different learning rates.')\n",
        "plt.legend() #让标签显示出来\n",
        "plt.show()\n",
        "# precision: TP/(TP+FP)\n",
        "#recall"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRwD5qxBjI7t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Q6ZdEqgq6F"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_LSTM_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,sent_encoded_train.shape[0],batch_size):\n",
        "        input_batch = sent_encoded_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])] #256,22,101\n",
        "        target_batch = label_train[ind:min(ind+batch_size, sent_encoded_train.shape[0])]  #256\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)  #256,22,101\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device) #256\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch)   #13,512,25 #256，22，101    #256 2 #500,22,101\n",
        "        loss = criterion(outputs, target_batch_torch) #256 2    256 out,y\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, 1)  # 输出值为元祖获取第二个tensor        \n",
        "    acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "    #print('Epoch: %d, train loss: %.5f, train_acc:%.2f'%(epoch + 1, train_loss, acc))\n",
        "\n",
        "#print('Finished Training')\n",
        "#这块是在后面评估，可删\n",
        "## Prediction\n",
        "\n",
        "model.eval()\n",
        "outputs2 = model(torch.from_numpy(sent_encoded_test).float().to(device)) \n",
        "predicted2 = torch.argmax(outputs2, 1)\n",
        "\n",
        "\n",
        "print(classification_report(label_test, predicted2.cpu().numpy(),digits=4))\n",
        "#是改变了batch"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}